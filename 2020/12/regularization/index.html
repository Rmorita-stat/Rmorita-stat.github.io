<!DOCTYPE html>
<html>
<head>
    <meta name="google-site-verification" content="qrt5G5NouQQVS-0Ss9qHlEuMYt3uKuifbUIOkPd4cPc" />
    <meta charset="utf-8">
    <title>
        
        SUCRE HECACHA
        
    </title>
    <meta name="viewport" id="viewport" content="width=device-width, initial-scale=1">
    <link rel='icon' type='image/x-icon' href="https://sucre-stat.com/images/favicon.png" />
    <link rel="apple-touch-icon" href="https://sucre-stat.com/images/favicon.png"><link rel="stylesheet" href="https://sucre-stat.com/scss/style.css">
    
    <link rel="stylesheet" href="https://sucre-stat.com/css/syntax.css">
    <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet">
    <script src="https://sucre-stat.com/js/highlight.min.js"></script>
    <link rel="stylesheet" href="https://sucre-stat.com/scss/highlight.css">
    
    <link rel="stylesheet" href="https://sucre-stat.com/scss/custom.css">
    
    
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'Your Google Analytics tracking id', 'auto');
        ga('send', 'pageview');
    </script>
    
    
    <meta name="generator" content="Hugo 0.71.0" /><link href="https://use.fontawesome.com/releases/v5.6.1/css/all.css" rel="stylesheet">
</head>


<body>
<div class="header">
    <div class="site-logo__wrap">
        <div id="site-button">
            <div></div>
        </div>
        
        <div class=' site-logo '>
            <a href="https://sucre-stat.com/"><img src="https://sucre-stat.com/images/logo.png" /></a>
        </div>
    </div>
    
<div class=' site-nav u-font ' id="nav-bar">
    
    <div class="site-nav__wrap">
        <a class="site-nav__el" href="https://sucre-stat.com/stat" >STAT</a>
    </div>
    
    <div class="site-nav__wrap">
        <a class="site-nav__el" href="https://sucre-stat.com/r" >With R</a>
    </div>
    
    <div class="site-nav__wrap">
        <a class="site-nav__el" href="https://sucre-stat.com/julia" >With Julia</a>
    </div>
    
    <div class="site-nav__wrap">
        <a class="site-nav__el" href="https://sucre-stat.com/about" >ABOUT</a>
    </div>
    
    <div class="site-nav__wrap">
        <a class="site-nav__el" href="https://sucre-stat.com/tags/" >TAGS</a>
    </div>
    
<div class="site-nav__wrap">
<a class="site-nav__el current-page" href="https://sucre-stat.com/search/" ><i class="fas fa-search my_small"></i>SEARCH</a>
</div>
<div class="site-nav__wrap">
<a class="site-nav__el current-page" href="https://sucre-stat.com/contact/" >CONTACT</a>
</div>

</div>
<div class="main">

<div class="main-content">
    <div class="main-content__date">
        <h4 id="date"> 2020.12.29 00:00 </h4>
    </div>
    <div class="main-content__title">
        <h1 id="title">正則化（Ridge回帰とLasso回帰）について</h1>
    </div>
    <div class="main-content__article">
        <article id="content">
            <h1 id="はじめに">はじめに</h1>
<p>今回は、回帰分析において過学習を防いだり、多重共線性に対応したりするために使われる<strong>Ridge回帰</strong>や、変数の数が標本数より多いような時に変数選択の方法として使われる<strong>Lasso回帰</strong>について、理論を整理しようと思います。</p>
<p>これら2つの手法ともには<strong>正則化</strong>という手法で説明されるものです。まずは正則化の観点からRidge回帰とLasso回帰を理解し、さらにこれらの手法が確率モデルでも説明可能であることを示そうと思います。</p>
<p>確率モデルで説明可能な手法であるということは、データセットとモデルから尤度を計算しベイズ推定可能であるということなので、ゆくゆくはRstanでこれら2つの手法を実装してみたいと思っています。</p>
<p>本記事の構成は以下の通りです。</p>
<!-- raw HTML omitted -->
<ul>
<li><a href="#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB">はじめに</a></li>
<li><a href="#%E7%B7%9A%E5%BD%A2%E5%9B%9E%E5%B8%B0%E3%81%AE%E5%B0%8E%E5%85%A5">線形回帰の導入</a></li>
<li><a href="#%E6%AD%A3%E5%89%87%E5%8C%96">正則化</a></li>
<li><a href="#%E7%A2%BA%E7%8E%87%E3%83%A2%E3%83%87%E3%83%AB%E3%81%A7ridge%E5%9B%9E%E5%B8%B0%E3%82%92%E6%8D%89%E3%81%88%E3%82%8B">確率モデルでRidge回帰を捉える</a></li>
<li><a href="#%E7%A2%BA%E7%8E%87%E3%83%A2%E3%83%87%E3%83%AB%E3%81%A7lasso%E5%9B%9E%E5%B8%B0%E3%82%92%E6%8D%89%E3%81%88%E3%82%8B">確率モデルでLasso回帰を捉える</a></li>
<li><a href="#%E8%9B%87%E8%B6%B3">蛇足</a></li>
</ul>
<!-- raw HTML omitted -->
<p>参考にした本や記事は以下のとおりです。</p>
<ul>
<li><a href="https://www.kspub.co.jp/book/detail/1529267.html">ガウス過程と機械学習</a></li>
<li><a href="https://zenn.dev/wsuzume/books/66b6fe7bb537b3e2b4bb/viewer/e646d3">リッジ回帰（Ridge Regression）</a></li>
<li><a href="https://zenn.dev/wsuzume/books/66b6fe7bb537b3e2b4bb/viewer/d86ddb">Lasso回帰（Lasso Regression）</a></li>
<li><a href="https://qiita.com/g-k/items/8b81e28ec3d1e6b8a62a">ラッソ回帰(L1正則化)を理解する(理論編) - Qiita</a></li>
</ul>
<h1 id="線形回帰の導入">線形回帰の導入</h1>
<p>以下の線形回帰を考えます。</p>
<div style="overflow-x:auto;">
  
$$
\hat{y} = w_0 + w_1\phi_1(\boldsymbol{x}) + \ldots, w_H\phi_H(\boldsymbol{x}) \tag{1}
$$

</div>
<p>ここで$\phi(\boldsymbol{x})$は$\boldsymbol{x}$の関数です。</p>
<p>これを行列で表現すると、</p>
<div style="overflow-x:auto;">
  
$$
\left(
\begin{array}{ccc}
\hat{y}_1 \\\\
\hat{y}_2 \\\\
\vdots \\\\
\hat{y}_N
\end{array}
\right) =
\left(
\begin{array}{ccc}
\phi_0(\boldsymbol{x}_1) & \phi_1(\boldsymbol{x}_1) & \cdots & \phi_H(\boldsymbol{x}_1) \\\\
\phi_0(\boldsymbol{x}_2) & \phi_1(\boldsymbol{x}_2) & \cdots & \phi_H(\boldsymbol{x}_2) \\\\
\cdots & \cdots & \ddots & \cdots \\\\
\phi_0(\boldsymbol{x}_N) & \phi_1(\boldsymbol{x}_N) & \cdots & \phi_H(\boldsymbol{x}_N)
\end{array}
\right)
\left(
\begin{array}{ccc}
w_0 \\\\
w_1 \\\\
\vdots \\\\
w_H
\end{array}
\right) \tag{2}
$$

</div>
<p>ただし、$\phi_0({\boldsymbol{x}_n}) = 1$です。</p>
<p>これを行列を使って以下のように表記します。</p>
<div style="overflow-x:auto;">
  
$$
\hat{\boldsymbol{y}} = \boldsymbol{\Phi}\boldsymbol{w} \tag{3}
$$

</div>
<p>一般に$\boldsymbol{\Phi}$を<strong>計画行列</strong>と呼びます。また$\phi_0(\boldsymbol{x}),\ldots,\phi_H(\boldsymbol{x})$を<strong>基底関数</strong>、$\boldsymbol{\phi}(\boldsymbol{x}) = \left(\phi_0(\boldsymbol{x}),\phi_1(\boldsymbol{x}),\ldots, \phi_H(\boldsymbol{x}) \right)^T$を$\boldsymbol{x}$の<strong>特徴ベクトル</strong>と呼びます。<br>
<br>
<div style="overflow-x:auto;">
  
<div class="memo" >
    <span class="memo-title">memo</span>
    <p>
基底関数を、ベクトル$\boldsymbol{x}$をスカラーに変換する解析的な関数であると想定していますが、
通常の重回帰モデルも$\boldsymbol{\phi}(\boldsymbol{x}) = \boldsymbol{x}$とすれば$(3)$式で説明できます！その場合基底関数は各変数それぞれを出力する関数になります。
</p>
</div>

</div></p>
<p>このとき、以下の公式が得られます。   <br>
<br>
<fieldset style="border: #b0e0e6 5px double; padding: 6px; background-color: #ffffff; margin: 0px; color: #333333; border-radius: 10px; box-shadow: 5px 5px 5px #AAAAAA"><legend><strong>◆線形回帰モデルの解</strong></legend>
線形回帰モデル$\hat{\boldsymbol{y}} = \boldsymbol{\Phi}\boldsymbol{w}$における$\boldsymbol{w}$の最小二乗解は
$$
\boldsymbol{w} = \left(\boldsymbol{\Phi}^T\boldsymbol{\Phi}\right)^{-1}\boldsymbol{\Phi}^T\boldsymbol{y} \tag{4}
$$
</fieldset><br>
<br>
<div style="overflow-x:auto;">
  
<fieldset style = "padding: 6px; background-color: #ffffff"><legend><strong>◆証明</strong></legend>

データ全体の誤差(損失関数)は、

$$
E = \sum_{n=1}^{N} \left( y_n - \boldsymbol{\phi}(\boldsymbol{x}_n)\boldsymbol{w} \right)^2 = (\boldsymbol{y}-\boldsymbol{\Phi}\boldsymbol{w})^T(\boldsymbol{y} - \boldsymbol{\Phi}\boldsymbol{w}) = \boldsymbol{y}^T\boldsymbol{y} - 2\boldsymbol{w}^T(\boldsymbol{\Phi}^T\boldsymbol{y}) + \boldsymbol{w}^T\boldsymbol{\Phi}\boldsymbol{\Phi}\boldsymbol{w} \tag{5}
$$

これを$\boldsymbol{w}$で微分すると、

$$
\cfrac{\partial E}{\partial\boldsymbol{w}} = -2\boldsymbol{\Phi}^T\boldsymbol{y} + 2\boldsymbol{\Phi}^T\boldsymbol{\Phi}\boldsymbol{w} \tag{6}
$$

よって、$E$を最小化する$\boldsymbol{w}$は、$\cfrac{\partial E}{\partial\boldsymbol{w}}=0$より下記の通り得られる。

$$
\boldsymbol{w} = \left(\boldsymbol{\Phi}^T\boldsymbol{\Phi}\right)^{-1}\boldsymbol{\Phi}^T\boldsymbol{y}
$$

</fieldset>

</div></p>
<h1 id="正則化">正則化</h1>
<p>線形回帰モデルの係数パラメータは$(4)$式で計算できることが分かりましたが、右辺には$\boldsymbol{\Phi}^T\boldsymbol{\Phi}$の逆行列が存在するため、$\boldsymbol{\Phi}^T\boldsymbol{\Phi}$が正則でない場合、係数パラメータの解を得ることが出来ません。<br>
<br>
<div style="overflow-x:auto;">
  
<div class="memo" >
    <span class="memo-title">memo</span>
    <p>
$n$次正方行列$\boldsymbol{A}$について、
$$
\boldsymbol{A}\boldsymbol{B} = \boldsymbol{B}\boldsymbol{A} = \boldsymbol{I}
$$
となる$n$次正方行列$\boldsymbol{B}$が存在するとき、$\boldsymbol{A}$は正則であるという。
</p>
</div>

</div></p>
<p>では、$\boldsymbol{\Phi}^T\boldsymbol{\Phi}$が正則でないのは具体的にどんな場合かとうと、$\boldsymbol{\phi}_ {i}(\boldsymbol{x}) = \boldsymbol{\phi}_ {j}(\boldsymbol{x})$、もしくは$\boldsymbol{\phi}_ {i}(\boldsymbol{x}) = \alpha\boldsymbol{\phi}_ {j}(\boldsymbol{x})~~(\alpha \in \mathbb{R})$となるような$\boldsymbol{\phi}(\boldsymbol{x})$の組が存在する場合が典型的です。このような場合、$\boldsymbol{\phi}_ {i}(\boldsymbol{x})$と$\boldsymbol{\phi}_ {j}(\boldsymbol{x})$の相関は1または-1になっています。また$\boldsymbol{\phi}(\boldsymbol{x}_ {i}) \simeq \alpha\boldsymbol{\phi}(\boldsymbol{x}_ {j})$だったりする場合では、$\boldsymbol{\Phi}^T\boldsymbol{\Phi}^{-1}$の要素が大きくなり、結果的に$\boldsymbol{w}$の絶対値が大きくなってしまい、過学習が生じるようです。このような場合では、$\boldsymbol{\phi}_ {i}(\boldsymbol{x})$と$\boldsymbol{\phi}_ {j}(\boldsymbol{x})$はかなり強い相関関係になっています<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</p>
<div style="overflow-x:auto;">
  
<div class="memo" >
    <span class="memo-title">memo</span>
    <p>
$\boldsymbol{\phi}_ {i}(\boldsymbol{x}) = \alpha\boldsymbol{\phi}_ {j}(\boldsymbol{x})~~(\alpha \in \mathbb{R})$となるような$\boldsymbol{\phi}(\boldsymbol{x})$の組が存在する場合、
$$
\mathrm{rank}\left(\boldsymbol{\Phi}^T\boldsymbol{\Phi}\right) < H
$$
と行列$\boldsymbol{\Phi}^T\boldsymbol{\Phi}$はランク落ちしており、ランク落ちした行列は逆行列が存在しない。
</p>
</div>

</div>
<p>係数パラメータの絶対値が大きくなるのを避けるための工夫が<strong>正則化</strong>です。正則化では、$(5)$式で得られたデータ全体の誤差に、$\boldsymbol{w}$の絶対値が大きくなることによるペナルティを課すように設定します。  このペナルティの設定方法が、Ridge回帰とLasso回帰の唯一の違いです。<br>
<br>
<fieldset style="border: #b0e0e6 5px double; padding: 6px; background-color: #ffffff; margin: 0px; color: #333333; border-radius: 10px; box-shadow: 5px 5px 5px #AAAAAA"><legend><strong>◆Ridge回帰とLasso回帰の損失関数</strong></legend>
Ridge回帰における損失関数は、
$$
E =  (\boldsymbol{y}-\boldsymbol{\Phi}\boldsymbol{w})^T(\boldsymbol{y} - \boldsymbol{\Phi}\boldsymbol{w}) + \alpha||\boldsymbol{w}||_2^2 \tag{7}
$$
Lasso回帰における損失関数は、
$$
E =  (\boldsymbol{y}-\boldsymbol{\Phi}\boldsymbol{w})^T(\boldsymbol{y} - \boldsymbol{\Phi}\boldsymbol{w}) + \alpha||\boldsymbol{w}||_1^1 \tag{8}
$$
ただし、$\alpha \geq 0$
</fieldset><br>
<br>
<div style="overflow-x:auto;">
  
<div class="memo" >
    <span class="memo-title">memo</span>
    <p>
$||\boldsymbol{x}||_p$はベクトルの$p$ノルムといい
$$
||\boldsymbol{x}||_p = \sqrt[p]{|x_1|^p+\ldots,|x_n|^p}
$$
を意味する。定義より$||\boldsymbol{x}||_1^1$は$\boldsymbol{x}$のマンハッタン距離を、$||\boldsymbol{x}||_2^2$は$\boldsymbol{x}$のユークリッド距離を意味する。
</p>
</div>

</div></p>
<p>損失関数を上記のように設定することが$\boldsymbol{\Phi}^T\boldsymbol{\Phi}$が正則でない場合に有効である仕組みを把握するために、Ridge回帰の場合についてみていきます。</p>
<p>$(7)$式を$\boldsymbol{w}$で微分すると、</p>
<div style="overflow-x:auto;">
  
$$
\cfrac{\partial E}{\partial\boldsymbol{w}} = -2\boldsymbol{\Phi}^T\boldsymbol{y} + 2\boldsymbol{\Phi}^T\boldsymbol{\Phi}\boldsymbol{w} + 2\alpha\boldsymbol{w}  \tag{9}
$$

</div>
<p>よって、$E$を最小化する$\boldsymbol{w}$は、$\cfrac{\partial E}{\partial\boldsymbol{w}}=0$より下記のとおり得られます。</p>
<div style="overflow-x:auto;">
  
$$
\boldsymbol{w} = \left(\boldsymbol{\Phi}^T\boldsymbol{\Phi} + \alpha\boldsymbol{I}\right)^{-1}\boldsymbol{\Phi}^T\boldsymbol{y} \tag{10}
$$

</div>
<p>リッジ回帰は、もともとは$(7)$式のように損失関数に$\boldsymbol{w}$の絶対値を小さくする為の項を設定したものでしたが、結果的に$\boldsymbol{\Phi}^T\boldsymbol{\Phi}$の対角要素に微小量$\alpha$を足すことで、逆行列計算の対象が正則行列であることを確実にし、計算を安定化させていることが分かります。<br>
<br>
<div style="overflow-x:auto;">
  
<div class="memo" >
    <span class="memo-title">memo</span>
    <p>
任意の行列$\boldsymbol{A} ∈ \mathbb{R}^{m×n}$、ベクトル$\boldsymbol{x} ∈ \mathbb{R}^m \neq \boldsymbol{0}$について、
$$
\boldsymbol{x}^T\left(\boldsymbol{A}\boldsymbol{A}^T\right)\boldsymbol{x} = \left(\boldsymbol{A}^T\boldsymbol{x}\right)^T\left(\boldsymbol{A}^T\boldsymbol{x}\right) = ||\boldsymbol{A}^T\boldsymbol{x}||_2^2 \geq 0
$$

だから、$\boldsymbol{A}\boldsymbol{A}^T$は半正定値行列。
また

$$
\boldsymbol{x}^T\left(\alpha\boldsymbol{I}\right)\boldsymbol{x} = \alpha||\boldsymbol{x}||_2^2 > 0
$$

だから、$\alpha\boldsymbol{I}$は正定値行列。このとき
$$
\boldsymbol{x}^T\left(\boldsymbol{A}\boldsymbol{A}^T + \alpha\boldsymbol{I}\right)\boldsymbol{x} = ||\boldsymbol{A}^T\boldsymbol{x}||_2^2 + \alpha||\boldsymbol{x}||_2^2 > 0
$$

よって$\boldsymbol{A}^T\boldsymbol{A} + \alpha\boldsymbol{I}$は正定値行列であり、正則行列。
</p>
</div>

</div></p>
<p>Lasso回帰については同様の方法では正則化との関係が見えてきませんでした。ただ係数パラメータの多くが0になる仕組みについては<a href="https://qiita.com/g-k/items/8b81e28ec3d1e6b8a62a">こちらの記事</a>に詳しかったです。</p>
<h1 id="確率モデルでridge回帰を捉える">確率モデルでRidge回帰を捉える</h1>
<p>前節で説明したRidge回帰は確率分布を用いた以下のモデルでも説明できます。</p>
<p><fieldset style="border: #b0e0e6 5px double; padding: 6px; background-color: #ffffff; margin: 0px; color: #333333; border-radius: 10px; box-shadow: 5px 5px 5px #AAAAAA"><legend><strong>◆Ridge回帰の確率モデル</strong></legend>
$(7)$のRidge回帰と等価の確率モデルは、
$$
\begin{cases}
p(y | \boldsymbol{w}, \boldsymbol{x}) = \mathrm{Normal}(y | \boldsymbol{w}^T\boldsymbol{\phi}(\boldsymbol{x}), \sigma^2) \\<br>
p(\boldsymbol{w}) = \mathrm{Normal}(\boldsymbol{w}|0, \rho^{-1}\boldsymbol{I}_H)
\end{cases} \tag{11}
$$
ただし、$\alpha = \rho\sigma^2$
</fieldset><br>
<br>
<div style="overflow-x:auto;">
  
<fieldset style = "padding: 6px; background-color: #ffffff"><legend><strong>◆証明</strong></legend>

$y$と$\boldsymbol{w}$の同時確率密度について以下の通り仮定する。
$$
p(y,\boldsymbol{w}|\boldsymbol{x}) = p(y | \boldsymbol{w},\boldsymbol{x})p(\boldsymbol{w})
$$
これを$\boldsymbol{w}$の関数$F(\boldsymbol{w})$とみたとき、独立同分布のデータセット$\boldsymbol{y}$、$\boldsymbol{X}=(\boldsymbol{x}_1,\ldots,\boldsymbol{x}_N)^T$がすべて観測された時の$F(\boldsymbol{w})$を最大化する$\boldsymbol{w}$が最尤推定解である。
$$
\newcommand{\argmax}{\mathop{\rm arg~max}\limits}
\newcommand{\argmin}{\mathop{\rm arg~min}\limits}
\begin{split}
\argmax_{\boldsymbol{w}∈\mathbb{R}^{H+1}} \log F(\boldsymbol{w}) &= \argmax_{\boldsymbol{w}∈\mathbb{R}^{H+1}} \log \prod_{n=1}^{N}p(y_n|\boldsymbol{w}, \boldsymbol{x}_ n)p(\boldsymbol{w})\\\\
&= \argmax_{\boldsymbol{w}∈\mathbb{R}^{H+1}}\left[ \sum_{n=1}^{N}\log\mathrm{Normal}(y_n| \boldsymbol{w}^T\boldsymbol{\phi}(\boldsymbol{x}),\sigma^2) + \log\mathrm{Normal}(\boldsymbol{w}|\boldsymbol{0}, \rho^{-1}\boldsymbol{I}_H)\right] \\\\
&= \argmax_{\boldsymbol{w}∈\mathbb{R}^{H+1}}\left[\log\mathrm{MultiNormal}(\boldsymbol{y}| \boldsymbol{\Phi}\boldsymbol{w},\sigma^2\boldsymbol{I}_N) + \log\mathrm{Normal}(\boldsymbol{w}|\boldsymbol{0}, \rho^{-1}\boldsymbol{I}_H)\right] \\\\
&= \argmax_{\boldsymbol{w}∈\mathbb{R}^{H+1}}\left[ -\cfrac{1}{2\sigma^2}\left(\boldsymbol{y} - \boldsymbol{\Phi}\boldsymbol{w}\right)^T\left(\boldsymbol{y} - \boldsymbol{\Phi}\boldsymbol{w}\right) -\cfrac{\rho}{2}\boldsymbol{w}^T\boldsymbol{w} + C \right] \\\\
&=\argmin_{\boldsymbol{w}∈\mathbb{R}^{H+1}}\left[\left(\boldsymbol{y} - \boldsymbol{\Phi}\boldsymbol{w}\right)^T\left(\boldsymbol{y} - \boldsymbol{\Phi}\boldsymbol{w}\right) + \rho\sigma^2||\boldsymbol{w}||_2^2\right] \\\\
&= \argmin_{\boldsymbol{w}∈\mathbb{R}^{H+1}}\left[\left(\boldsymbol{y} - \boldsymbol{\Phi}\boldsymbol{w}\right)^T\left(\boldsymbol{y} - \boldsymbol{\Phi}\boldsymbol{w}\right) + \alpha||\boldsymbol{w}||_2^2\right]
\end{split}
$$
これは$(7)$式のRidge回帰における損失関数$E$の$\boldsymbol{w}$についての最小化である。

</fieldset>

</div></p>
<h1 id="確率モデルでlasso回帰を捉える">確率モデルでLasso回帰を捉える</h1>
<p>Lasso回帰は確率分布を用いた以下のモデルでも説明できます。</p>
<p><fieldset style="border: #b0e0e6 5px double; padding: 6px; background-color: #ffffff; margin: 0px; color: #333333; border-radius: 10px; box-shadow: 5px 5px 5px #AAAAAA"><legend><strong>◆Lasso回帰の確率モデル</strong></legend>
$(8)$のLasso回帰と等価の確率モデルは、
$$
\begin{cases}
p(y | \boldsymbol{w}, \boldsymbol{x}) = \mathrm{Normal}(y | \boldsymbol{w}^T\boldsymbol{\phi}(\boldsymbol{x}), \sigma^2) \\<br>
p(\boldsymbol{w}) = \prod_{h=1}^{H}\mathrm{Laplace}(w_h | 0, b)
\end{cases} \tag{12}
$$
ただし、$\alpha = \cfrac{2\sigma^2}{b}$
</fieldset><br>
<br>
<div style="overflow-x:auto;">
  
<fieldset style = "padding: 6px; background-color: #ffffff"><legend><strong>◆証明</strong></legend>

$y$と$\boldsymbol{w}$の同時確率密度について以下の通り仮定する。
$$
p(y,\boldsymbol{w}|\boldsymbol{x}) = p(y | \boldsymbol{w},\boldsymbol{x})p(\boldsymbol{w})
$$
これを$\boldsymbol{w}$の関数$F(\boldsymbol{w})$とみたとき、独立同分布のデータセット$\boldsymbol{y}$、$\boldsymbol{X}=(\boldsymbol{x}_1,\ldots,\boldsymbol{x}_N)^T$がすべて観測された時の$F(\boldsymbol{w})$を最大化する$\boldsymbol{w}$が最尤推定解である。
$$
\newcommand{\argmax}{\mathop{\rm arg~max}\limits}
\newcommand{\argmin}{\mathop{\rm arg~min}\limits}
\begin{split}
\argmax_{\boldsymbol{w}∈\mathbb{R}^{H+1}} \log F(\boldsymbol{w}) &= \argmax_{\boldsymbol{w}∈\mathbb{R}^{H+1}} \log \prod_{n=1}^{N}p(y_n|\boldsymbol{w}, \boldsymbol{x}_ n)p(\boldsymbol{w})\\\\
&= \argmax_{\boldsymbol{w}∈\mathbb{R}^{H+1}}\left[ \sum_{n=1}^{N}\log\mathrm{Normal}(y_n| \boldsymbol{w}^T\boldsymbol{\phi}(\boldsymbol{x}),\sigma^2) + \sum_{h=1}^{H}\log\mathrm{Laplace}(w_h|0, b)\right] \\\\
&= \argmax_{\boldsymbol{w}∈\mathbb{R}^{H+1}}\left[\log\mathrm{MultiNormal}(\boldsymbol{y}| \boldsymbol{\Phi}\boldsymbol{w},\sigma^2\boldsymbol{I}_ N) + \sum_{h=1}^{H}\log\mathrm{Laplace}(w_h|0, b)\right] \\\\
&= \argmax_{\boldsymbol{w}∈\mathbb{R}^{H+1}}\left[ -\cfrac{1}{2\sigma^2}\left(\boldsymbol{y} - \boldsymbol{\Phi}\boldsymbol{w}\right)^T\left(\boldsymbol{y} - \boldsymbol{\Phi}\boldsymbol{w}\right) - \cfrac{1}{b}\sum_{h=1}^{H}|w_h| + C \right] \\\\
&=\argmin_{\boldsymbol{w}∈\mathbb{R}^{H+1}}\left[\left(\boldsymbol{y} - \boldsymbol{\Phi}\boldsymbol{w}\right)^T\left(\boldsymbol{y} - \boldsymbol{\Phi}\boldsymbol{w}\right) + \cfrac{2\sigma^2}{b}||\boldsymbol{w}||_1^1\right] \\\\
&= \argmin_{\boldsymbol{w}∈\mathbb{R}^{H+1}}\left[\left(\boldsymbol{y} - \boldsymbol{\Phi}\boldsymbol{w}\right)^T\left(\boldsymbol{y} - \boldsymbol{\Phi}\boldsymbol{w}\right) + \alpha||\boldsymbol{w}||_1^1\right]
\end{split}
$$
これは$(8)$式のLasso回帰における損失関数$E$の$\boldsymbol{w}$についての最小化である。

</fieldset>

</div>
<br>
<div style="overflow-x:auto;">
  
<div class="memo" >
    <span class="memo-title">memo</span>
    <p>
確率密度$p(x)$が
$$
p(x) = \cfrac{1}{2b}\exp\left[-\cfrac{|x - \mu|}{b}\right]
$$
である確率分布をラプラス分布といい、確率変数$X$がラプラス分布に従うことを
$$
X \sim \mathrm{Laplace}(\mu, b)
$$
と表す。
</p>
</div>

</div></p>
<h1 id="蛇足">蛇足</h1>
<p>今回の記事は<a href="https://gohugo.io/content-management/shortcodes/">Hugoのshortcode機能</a>を駆使して文章に枠をもたせてみました。今までの記事よりかなり見やすくなっていませんか？？しばらくはこのスタイルで行こうと思います。</p>
<p>最後に今回の記事のイメージソングをあげておきます。ブログ名の由来になった曲です。</p>

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/OCNExXuks8A" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>回帰分析で生じるこのような問題は<strong>多重共線性</strong>と呼ばれています。 <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

        </article>
    </div>
    <div class="main-content__tags u-font">
        
        
        <span><a href="https://sucre-stat.com/tags/%E5%9B%9E%E5%B8%B0">回帰</a></span>
        
        
    </div>

<div class="contact_form">
    <p>コメントを書く</p>
<form className='contact_form' id="my-form" action="https://sucre-stat.com/" name="sampleCommentForm" method="post" netlify-honeypot="bot-field" netlify>
    <label>お名前：<input type="text" className="contact__field" name="name" /></label>
    <label>Email(任意)： <input type="email" className="contact__field" name="email" /></label>
    <label>コメント： <textarea name="content" className="contact__field"></textarea></label>
  <p style="display:none;">
    <label>Bot Field(ここに値が入るとスパム判定する): <input name="bot-field"/></label>
  </p>
  <div>
    <button type="submit" value="Send">送信</button>
    <p><br>※ コメントは承認されると表示されます<br></p>
    <input type="hidden" name="path" value="/2020/12/regularization/index.html" />
    <p>承認されたコメント一覧</p>
    <ul class="comment" id="approvalCommentsList"></ul>

  </div>
</form>
</div>
<div class="main-profile">
    <div class="main-profile__avatar">
        
            <img src="https://sucre-stat.com/images/avatar.JPG">
        
    </div>
    <div class="main-profile__body">
        <div class="main-profile__author">
            
            <span> R.morita </span>
            
        </div>
        <div class="main-profile__description">
            
            <p> 得意なのは統一された洋服とダンスのステップです </p>
            
        </div>
    </div>
</div>
<div class="main-line"></div>
<div class="main-pn">
    
    <a class="previous" href="https://sucre-stat.com/2020/07/gaussianprocess2/">
        <div class="pn-dec"></div>
        <div class="pn-els">
            <div class="pn-el__1"> 2020.07.11 00:00 </div>
            <div class="pn-el__2"> ガウス過程の応用 </div>
        </div>
    </a>
    
    
    <a class="next" href="https://sucre-stat.com/2020/12/information-criterion/">
        <div class="pn-dec"></div>
        <div class="pn-els">
            <div class="pn-el__1"> 2020.12.30 00:00 </div>
            <div class="pn-el__2"> モデル選択基準 </div>
        </div>
    </a>
    
</div>

<footer>
  <script type="text/javascript">
 MathJax = {
   tex: {
     inlineMath: [['$','$'], ['\\(','\\)']],
     processEscapes: true,
     tags: "ams",
     autoload: {
       color: [],
       colorV2: ['color']
     },
     packages: {'[+]': ['noerrors']}
   },
   chtml: {
     matchFontHeight: true,
     displayAlign: "left",
     displayIndent: "2em"
   },
   options: {
     skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
     renderActions: {
        
       find_script_mathtex: [10, function (doc) {
         for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
           const display = !!node.type.match(/; *mode=display/);
           const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
           const text = document.createTextNode('');
           node.parentNode.replaceChild(text, node);
           math.start = {node: text, delim: '', n: 0};
           math.end = {node: text, delim: '', n: 0};
           doc.math.push(math);
         }
       }, '']
     }
   },
   loader: {
     load: ['[tex]/noerrors']
   }
 };
</script>
<script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" id="MathJax-script"></script>
</footer>

</div>
<div class="footer">
    <div class="copyright-wrap">
        <p class="copyright u-font">
            
            &#169;
            2021
            
            <a href="https://github.com/Rmorita-stat/doc" target="_blank">R.morita&#46;</a>
            Theme <a href="https://github.com/iCyris/hugo-theme-yuki" target="_blank">yuki</a>&#46;
            Powered by Hugo&#46;
            
            
        </p>
    </div>
</div>
</body>
<script src="https://sucre-stat.com/js/page.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script src="//cdn.jsdelivr.net/npm/sweetalert2@11"></script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
<script language="javascript" type="text/javascript">

const url = '/comment/_data/sampleApprovedComments_submissions.json'
$.getJSON(url, (json) => {
  for (let i = 0; i < json.length; i++) {
    if (json[i].data.path !== '\/2020\/12\/regularization\/index.html') {
      continue
    }
    $('#approvalCommentsList').append(`<li>お名前　： ${json[i].data.name}<br>コメント： ${json[i].data.content}</li>`)
  }
})

// 送信前に一言感謝
$("#my-form").submit(function(e) {
  alert("Thank you!");
});
</script>
</body>
</html>
