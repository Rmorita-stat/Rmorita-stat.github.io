<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>
        
        SUCRE HECACHA
        
    </title>
    <meta name="viewport" id="viewport" content="width=device-width, initial-scale=1">
    <link rel='icon' type='image/x-icon' href="https://rmorita-stat.github.io/my-page/images/logo2.png" />
    <link rel="apple-touch-icon" href="https://rmorita-stat.github.io/my-page/images/logo2.png"><link rel="stylesheet" href="https://rmorita-stat.github.io/my-page/scss/style.css">
    
    <link rel="stylesheet" href="https://rmorita-stat.github.io/my-page/scss/monokai-sublime.min.css">
    <script src="https://rmorita-stat.github.io/my-page/js/highlight.min.js"></script>
    <link rel="stylesheet" href="https://rmorita-stat.github.io/scss/highlight.css">
    
    <link rel="stylesheet" href="https://rmorita-stat.github.io/scss/custom.css">
    
    
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'Your Google Analytics tracking id', 'auto');
        ga('send', 'pageview');
    </script>
    
    
    <meta name="generator" content="Hugo 0.71.0" /></head>


<body>
<div class="header">
    <div class="site-logo__wrap">
        <div id="site-button">
            <div></div>
        </div>
        
        <div class=' site-logo '>
            <a href="https://rmorita-stat.github.io/my-page/"><img src="https://rmorita-stat.github.io/my-page/images/logo.png" /></a>
        </div>
    </div>
    
<div class=' site-nav u-font ' id="nav-bar">
    
    <div class="site-nav__wrap">
        <a class="site-nav__el" href="/my-page/" >HOME</a>
    </div>
    
    <div class="site-nav__wrap">
        <a class="site-nav__el" href="/my-page/post" >BLOG</a>
    </div>
    
    <div class="site-nav__wrap">
        <a class="site-nav__el" href="/my-page/about" >ABOUT</a>
    </div>
    
    <div class="site-nav__wrap">
        <a class="site-nav__el" href="/my-page/tags/" >TAGS</a>
    </div>
    
</div>

</div>
<div class="main">

<div class="main-content">
    <div class="main-content__date">
        <h4 id="date"> 2020.06.28 00:00 </h4>
    </div>
    <div class="main-content__title">
        <h1 id="title">多項ロジスティック回帰・ガウス過程モデル</h1>
    </div>
    <div class="main-content__article">
        <article id="content">
            <h1 id="はじめに">はじめに</h1>
<p><strong>回帰</strong>に関する記事です。</p>
<p><a href="https://rmorita-stat.github.io/my-page/post/multinom/multinom/">以前の記事</a>では、対象が3つ以上のクラスに分類されるとき、それぞれのクラスに属する確率を予測するモデルである<strong>多項ロジスティック回帰</strong>について書きました。</p>
<p><a href="https://rmorita-stat.github.io/my-page/post/multinom-rstan/multinom-rstan/">次の記事</a>では、同様のモデルを、<strong>マルコフ連鎖モンテカルロ法</strong>(MCMC)を実行するための汎用ソフトである<strong>Stan</strong>を用いて実装し、<strong>ベイズ統計</strong>の立場から分析しました。MCMCおよびベイズ統計については<a href="https://rmorita-stat.github.io/my-page/post/bayesintroduction/bayesintroduction/">こちらの記事</a>で(走り書きで)書いています。</p>
<p>上で見た記事ではいずれも対象が各クラスに属する確率のオッズ比に着目し、オッズ比の線形性のみを考慮したモデルです。このように統計モデルでは変数の背後に何かしらの分布を想定したり、変数間に線形的な関係を想定したりすることでモデルを構築します。</p>
<p>一方で、特定の分布や関係を仮定せず、与えられたデータに柔軟に対応することのできるモデルも存在します。そのようなモデルの例として<a href="https://rmorita-stat.github.io/my-page/post/gaussianprocessori/gaussianprocessori/">前回の記事</a>では<strong>ガウス過程</strong>回帰について取り上げました。</p>
<p>今回の記事はこれらの記事の内容をフル活用し、多項ロジスティック回帰にガウス過程を適用したモデルを実装します。ちなみにガウス過程のように特定の分布を想定しないで、データに応じてモデルの複雑さを決定するパラメータを調整するベイズモデルは<strong>ノンパラメトリックベイズモデル</strong>と呼ばれています。</p>
<p>最終目標は、対象が各クラスに属する確率の予測値を、データに柔軟にフィットさせることです。</p>
<h1 id="データの入手">データの入手</h1>
<p>これまでと同様に、ある学校の生徒の属性と各生徒が選択した授業に関するデータを用います。</p>
<pre><code>library(foreign)
ml &lt;- read.dta(&quot;https://stats.idre.ucla.edu/stat/data/hsbdemo.dta&quot;)
head(ml)

##    id female    ses schtyp     prog read write math science socst       honors
## 1  45 female    low public vocation   34    35   41      29    26 not enrolled
## 2 108   male middle public  general   34    33   41      36    36 not enrolled
## 3  15   male   high public vocation   39    39   44      26    42 not enrolled
## 4  67   male    low public vocation   37    37   42      33    32 not enrolled
## 5 153   male middle public vocation   39    31   40      39    51 not enrolled
## 6  51 female   high public  general   42    36   42      31    39 not enrolled
##   awards cid
## 1      0   1
## 2      0   1
## 3      0   1
## 4      0   1
## 5      0   1
## 6      0   1
</code></pre>
<p>今回も、general, academic, vocation の3つの授業の中から生徒が選択した結果(prog)を出力（目的変数）、write(書く力を得点化したもの)と家庭の経済状況の指標ses(low, middle, highに分類)の2変数を入力(説明変数)とし、モデルを組み立てていきます。</p>
<h1 id="必要な知識小技の補完">必要な知識・小技の補完</h1>
<p>今回組み立てるモデルの理解に必要な、ガウス過程にまつわる知識について補完していきます。</p>
<h5 id="ガウス過程の導入">ガウス過程の導入</h5>
<p>まず、ガウス過程を用いたモデルについて一般化しておきます。
<!-- raw HTML omitted -->  <br>
入力の要素数を数を$I$とし、入力データの1サンプルを$\mathrm{x} = (x_1, \ldots, x_I)^T$と表記し、全入力を$N × I$行列$\mathrm{X}$を用いて$\mathrm{X} = (\mathrm{x_1}, \ldots, \mathrm{x_N})^T$とおきます。また出力データを$\mathrm{y} = (y_1, \ldots, y_N)^T$とおきます。</p>
<p>ガウス過程モデルの一般式は以下になります。</p>
<p>$$
y(\mathrm{x}) = \mathrm{\beta}\mathrm{f}(\mathrm{x}) + \epsilon(\mathrm{x}) \tag{1}
$$</p>
<p>ここで$\mathrm{f} = (f_1(\mathrm{x}), \ldots, f_l(\mathrm{x}))$、$\mathrm{\beta} = (\beta_1, \ldots, \beta_l )$はそれぞれ、ガウス過程モデルとは別で設定された特徴ベクトルとその重みです。これらはガウス過程を適用する以前からデータにあてはめられたモデルになります。</p>
<p>$\epsilon(\mathrm{x})$は右辺第一項のモデルで説明しきれなかった残差としての扱いですが、この残差がガウス過程に従うと仮定します。ガウス過程モデルは得られたデータに柔軟に近づこうとする挙動をとりますので、この性質を利用して右辺第一項で説明できなかった部分をガウス過程で補おうという目論見です。</p>
<p>全入力が共通の標準偏差$\sigma$をもつように設定しておくと、</p>
<p>$$
\epsilon(\mathrm{x}) \sim   Normal(0, \sigma^2\mathrm{K}) \tag{2}
$$</p>
<p>上式では$\epsilon$が残差を説明するものであることから多変量正規分布の平均を0としています。また分散共分散行列$\mathrm{K}$は、ハイパーパラメータ${\phi}$を持つカーネル関数$k_{\phi}(\mathrm{x}_m, \mathrm{x}_n)$を要素に持つ$N$×$N$行列です($m,n = 1,\ldots,N$)。</p>
<p>$$
\mathrm{K}(m,n) = k_{\phi}(\epsilon(\mathrm{x}_m), \epsilon(\mathrm{x}_n)) \tag{3}
$$</p>
<p>各入力の標準偏差を$\sigma$で設定している為、$\mathrm{K}$の$(m,n)$成分は、$\epsilon(\mathrm{x}_m)$と$\epsilon(\mathrm{x}_n)$の相関に等しくなり、</p>
<p>$$
k_{\phi}(\epsilon(\mathrm{x}_m), \epsilon(\mathrm{x}_n)) = cor(\epsilon(\mathrm{x}_m), \epsilon(\mathrm{x}_n)) \tag{4}
$$</p>
<p>となります。</p>
<p>ガウス過程では、カーネル関数によって$\mathrm{x}_m$と$\mathrm{x}_n$の距離を定める空間を決定し、入力$\mathrm{x}_m$と$\mathrm{x}_n$の距離によって出力$\epsilon(\mathrm{x}_m)$と$\epsilon(\mathrm{x}_n)$の近さを決定します。また、カーネル関数によって決まる値は、$cor(\epsilon(\mathrm{x}_m),\epsilon(\mathrm{x}_n))$と解釈することができる、ということです。</p>
<h5 id="多変量正規分布のサンプリング">多変量正規分布のサンプリング</h5>
<p>$(2)$式で用いられる多変量正規分布について、ランダムにサンプルを得る方法を紹介します。</p>
<p>平均0、分散共分散行列$\Sigma$の多変量正規分布からのサンプルを得る場合、まず</p>
<p>$$
\mathrm{\Sigma} = \mathrm{L}\mathrm{L}^T \tag{5}
$$</p>
<p>を満たす行列$L$を求めます。$(4)$式のような行列の分解は<strong>コレスキー分解</strong>と呼ばれます。</p>
<p>次に、標準正規分布からの乱数$\mathrm{x} = (x_1, \ldots, x_N)$を生成します。</p>
<p>$$
x_n \sim Normal(0,1) ~~~ (n = 1, \ldots, N) \tag{6}
$$</p>
<p>$\mathrm{y} = \mathrm{L}\mathrm{x}$の分布は、</p>
<p>$$
p(\mathrm{x}) = \cfrac{1}{(\sqrt{2\pi}^N \sqrt{det\Sigma})}exp\left(-\cfrac{1}{2}(\mathrm{x}^T \mathrm{I}^{-1} \mathrm{x}) \right)  \propto exp\left(-\cfrac{1}{2}\mathrm{x}^T\mathrm{I}^{-1}\mathrm{x}\right) \tag{7}
$$</p>
<p>に$\mathrm{x} = \mathrm{L}^{-1}\mathrm{y}$を代入すると、変数変換による空間の単位当たり面積の変動を調整するヤコビアン$|\partial{\mathrm{y}}/\partial{\mathrm{x}}|$は定数だから、</p>
<p>$$
p(\mathrm{Lx})  \propto exp\left(-\cfrac{1}{2}\left(\mathrm{L}^{-1}\mathrm{y}\right)^T \mathrm{I}^{-1}\mathrm{L}^{-1}\mathrm{y}\right)
\left|\cfrac{\partial{\mathrm{y}}}{\partial{\mathrm{x}}}\right|
= exp\left(-\cfrac{1}{2}\mathrm{y}^{-1}\left(\mathrm{L}^{-1}\right)^T\mathrm{L}^{-1}\mathrm{y}\right)
= exp\left(-\cfrac{1}{2}\mathrm{y}^{-1}\left(\mathrm{L}\mathrm{L}^T\right)^{-1}\mathrm{y}\right)
= exp\left(-\cfrac{1}{2}\mathrm{y}^T\mathrm{\Sigma}^{-1}\mathrm{y}\right) \tag{8}
$$</p>
<p>となります。</p>
<p>このことから、$Normal(0,\Sigma)$に従う乱数を生成するには、標準正規分布に従う$\mathrm{x}$をランダムに生成し、$y=\mathrm{Lx}$と変換すればよいと分かります。</p>
<h5 id="ガウス過程潜在変数モデルのstanでの実装">ガウス過程潜在変数モデルのStanでの実装</h5>
<p>モデルの残差$\epsilon(\mathrm{x})$がガウス過程に従うとした$(2)$式を、先ほど紹介した多変量正規分布の乱数生成法を用いて変形すると、</p>
<p>$$
\mathrm{K} = \mathrm{L}\mathrm{L}^T
$$
$$
\eta_n \sim Normal(0,1) \tag{9}
$$
$$
\epsilon({\mathrm{x}_n}) = \mathrm{L}\eta_n
$$</p>
<p>となります。
このように、潜在変数$w_n$を用いたガウス過程は、<strong>ガウス過程潜在変数モデル</strong>(Latent variable GP)と呼ばれ、出力が正規分布でないとき等に有用です。</p>
<p>以下、<a href="https://mc-stan.org/docs/2_22/stan-users-guide/fit-gp-section.html">Stanマニュアル</a>を引用してLatent variable GPの実装について軽く触れておきます。</p>
<p>Latent variable GPのStanでの実装は以下のようになります。</p>
<pre><code>data{
  int&lt;lower=1&gt; N;
  real x[N];
  vector[N] y;
}
    
transformed data{
  real delta = 1e-9;
}
    
parameters {
  real&lt;lower=0&gt; rho;
  real&lt;lower=0&gt; alpha;
  real&lt;lower=0&gt; sigma;
  vector[N] eta;
}
    
model {
  vector[N] f;
  {
    matrix[N, N] L_K;
    matrix[N, N] K = cov_exp_quad(x, alpha, rho);
    
    // diagonal elements
    for (n in 1:N)
      K[n, n] = K[n, n] + delta;
    
    L_K = cholesky_decompose(K);
    f = L_K * eta;
  }
  eta ~ std_normal();
  y ~ normal(f, sigma);
}
</code></pre>
<p>ここで、<code>K = cov_exp_quad(x, alpha, rho)</code>はガウスカーネルをつくる便利な関数で、</p>
<p>$$
\mathrm{K}(m,n) =k_{\alpha, \rho}(\epsilon(\mathrm{x}_m),\epsilon(\mathrm{x}_n))
=\alpha^2exp\left(-\cfrac{1}{2\rho^2}\sum_{i=1}^{I}(\mathrm{x}_{m,i}-\mathrm{x}_{n,i})^2\right)  \tag{10}
$$</p>
<p>を要素に持つカーネル行列$\mathrm{K}$を作成してくれます。</p>
<p>また、<code>for (n in 1:N) K[n, n] = K[n, n] + delta;</code>とすることで、カーネル行列の対角要素に微小な値を加えていますが、こうすることでカーネル行列の逆行列の計算を安定化させています。また、コレスキー分解はその対象が正定値行列であることが必須ですが、対角要素に微小量を加えることで、その行列が正定値行列であることを保証することができます。</p>
<p>式$(9)$の3式の計算は、それぞれ<code>L_K = cholesky_decompose(K);</code>、<code>eta ~ std_normal();</code>、<code>f = L_K * eta;</code>と指定しています。</p>
<p>上のコードでは、残差ではなく出力$\mathrm{y}$の平均値が$\mathrm{f} \sim Normal(0, \mathrm{K}_{\phi})$に従うと設定し、<code>y ~ Normal(f, sigma)</code>とすることで、平均$\mathrm{f}$の正規分布に従うと設定していますが、この部分をポアソン分布等他の分布にすることで、様々なモデルを構築することが出来ます。</p>
<p>例えば、0か1のみをとる出力$\mathrm{y}$にベルヌーイ分布を仮定し、$y_n=1$となる確率$\mathrm{p} = (p_1, \dots, p_N)$を、入力$\mathrm{x}_n$のガウス過程を用いて説明する場合、</p>
<p>$$
\mathrm{y} \sim Binomial(\mathrm{p})
$$
$$
\mathrm{p} = inverselogit(\mathrm{f})　\tag{11}
$$
$$
\mathrm{f} \sim Normal(\mu, \mathrm{K})
$$</p>
<p>と表現でき、これをStanで実行する場合、以下のようになります。</p>
<pre><code>data{
  int&lt;lower=1&gt; N;
  real x[N];
  vector[N] y;
  ...
}

parameters{
  real mu; //muはpの期待値 観測データが近くに無い場合に漸近する値
  ...
}

transformed parameters{
  vector[N]&lt;lower=0, upper=1&gt; p;
  ...
  p = mu + f; 
  ...
}
...
model {
  mu ~ std_normal()
  ...
  y ~ bernoulli(p);
}
</code></pre>
<h5 id="ガウス過程の予測分布">ガウス過程の予測分布</h5>
<p>ガウス過程モデルにおいて、入力$\mathrm{X}$に含まれない値$\mathrm{X}^* = (\mathrm{x}^*_1,\ldots,\mathrm{x}_N^*)$に対応する出力の値$\mathrm{y}^* = (y^*_1, \ldots, y^*_M)$を予測したい場合、$\mathrm{y}$と$\mathrm{y}^*$の同時分布を次のようにすればよいです。</p>
<p>$$
\left(
\begin{array}{ccc}
y_1 \\<br>
\vdots \\<br>
y_N \\<br>
y_1^* \\<br>
\vdots \\<br>
y_M^*
\end{array}
\right) \sim Normal \left(\vec{0},
\left( 
\begin{array}{ccc}
\mathrm{K} &amp; \mathrm{k}^* \\<br>
\mathrm{k}^{*T} &amp; \mathrm{k}^{**}  \\<br>
\end{array}
\right)\right) \tag{12}
$$</p>
<p>ここで、$\mathrm{k}^*(n,m) = k_{\phi}(\mathrm{x}_n, \mathrm{x}^*_m)$、$\mathrm{k}^{**}(m,m) = k_{\phi}(\mathrm{x}_m^*, \mathrm{x}^*_m)$です。</p>
<h1 id="カテゴリカルな変数を用いたカーネル">カテゴリカルな変数を用いたカーネル</h1>
<p>今回対象にするデータについて、writeは量的変数でしたが、sesは連続的な値をとらず、絶対的な大小関係をもたない<a href="https://toukeigaku-jouhou.info/2019/06/09/qualitative-and-quantitative-data/">質的変数</a>で、特に(low、middle、high)の3つの独立した値をとる、<strong>カテゴリカルな変数</strong>でした。このように、量的変数と質的変数が入力に混在する場合、両方の性質を考慮した空間を定義することのできるカーネルを設定する必要があります。しかし、質的変数には「距離」の概念が無いため、ガウスカーネル等のように、各入力の「近接性」を再現するカーネルで対応することはできません。では、どのようにカーネル関数を設定すればよいのでしょうか。</p>
<h5 id="質的変数が1つの場合">質的変数が1つの場合</h5>
<p>まず、量的変数と質的変数を含んだ入力を$\mathrm{w} = (\mathrm{x}^T, \mathrm{z}^T)^T$とし、$\mathrm{x} = (x_1, \ldots, x_I)$、を量的変数$\mathrm{z} = (z_1, \ldots, z_J)$を質的変数とします。それに従い、$(1)$式、$(2)$式を</p>
<p>$$
y(\mathrm{w}) = \mathrm{\beta}\mathrm{f}(\mathrm{w}) + \epsilon(\mathrm{w}) \tag{1&rsquo;}
$$</p>
<p>$$
\epsilon(\mathrm{z}) \sim  Normal(0, \sigma^2\mathrm{K}) \tag{2&rsquo;}
$$</p>
<p>としておきます。</p>
<p>簡略のため、$m_1$個の値をとる一つの質的変数$z_1$について考えます。
$\mathrm{w}= (\mathrm{x}^T, z_1) = (\mathrm{x}^T, u)$($u = 1,\ldots,m_1$)における$\mathrm{\epsilon}(\mathrm{x})$を、</p>
<p>$$
\mathrm{\epsilon}^*(\mathrm{x}) = \left(
\begin{array}{ccc}
\epsilon_1(\mathrm{x}) \\<br>
\vdots \\<br>
\epsilon_{m_1}(\mathrm{x}) 
\end{array}
\right) \tag{13}
$$</p>
<p>と定義します。すると、$\epsilon^*(\mathrm{x})$の各要素については、量的変数に関するカーネル関数で空間的距離を指定し、質的変数の影響については、$\epsilon^*(\mathrm{x})$の分散共分散行列を決定すれば、量的変数・質的変数双方の差異を考慮した相関関数$cor(\epsilon(\mathrm{w}_m),\epsilon(\mathrm{w}_n))$を指定できそうです。そこで、$\epsilon^*(\mathrm{x})$を</p>
<p>$$
\epsilon(\mathrm{x})^* = \mathrm{A}\mathrm{\eta}(\mathrm{x}) \tag{14}
$$</p>
<p>と推定することにします。</p>
<p>ここで、$\mathrm{\eta}(\mathrm{x}) = (\eta_1(\mathrm{x}), \ldots, \eta_{m1}(\mathrm{x}))^T$は量的変数における「距離」を考慮する部分で、各要素がそれぞれ独立に、標準偏差$\sigma$、相関関数$\mathrm{K}$のガウス過程に従って生成されるものとします。</p>
<p>また、$m_1×m_1$行列$\mathrm{A}$は質的変数の影響を考慮する部分で、単位行ベクトル$a_u$($a_ua_u^T = 1$、$u = 1, \ldots, m_1$)を用いて</p>
<p>$$
\mathrm{A} = \left(
\begin{array}{ccc}
a_1 \\<br>
\vdots \\<br>
a_{m1} 
\end{array}
\right) \tag{15}
$$</p>
<p>とし、$\epsilon_i(\mathrm{x})$($i = 1,\ldots, m_1$)を単位行ベクトル$a_i$の指定する重みに基づく$\eta(\mathrm{x})$の線形和で表現することにします。</p>
<p>すると、</p>
<p>$$
cor(\eta_{z_1m}(\mathrm{x}_n),\eta_{z_1n}(\mathrm{x}_m))=\left(
\begin{array}{ccc}
k_{\phi}(\mathrm{x}_m,\mathrm{x}_n)&amp;0&amp;\cdots&amp;0 \\<br>
0&amp;k_{\phi}(\mathrm{x}_m,\mathrm{x}_n)&amp;\cdots&amp;0\\<br>
\vdots&amp; \vdots&amp; \ddots&amp; \vdots \\<br>
0&amp;0&amp;\cdots&amp;k_{\phi}(\mathrm{x}_m,\mathrm{x}_n)
\end{array}
\right) \tag{16}
$$</p>
<p>($\eta_{i}(\mathrm{x}_n)$、$\eta_{i&rsquo;}(\mathrm{x}_n)$は$i=i'$のときのみ$k_{\phi}()$の相関をとる)だから、2つの入力$\mathrm{w}_i = (\mathrm{x}_i^T, z_i)^T$($i=m,n$)の相関関数$cor(\epsilon(\mathrm{w}_m), \epsilon(\mathrm{w}_n))$について、</p>
<p>$$
a_{z_1m}a_{z_1n}^Tk_{\phi}(\mathrm{x}_m, \mathrm{x}_n)=
cor(a_{z_1m}\eta(\mathrm{x}_m),a_{z_1n}\eta(\mathrm{x}_n))=
cor(\epsilon_{z_1m}(\mathrm{x}_m),\epsilon_{z_1n}(\mathrm{x}_n))=
cor(\epsilon(\mathrm{w}_m),\epsilon(\mathrm{w}_n))\tag{17}
$$</p>
<p>が成り立ちます($zm,zn = 1, \ldots,m1$)。</p>
<p>ここで、$\tau_{r,s} = a^T_ra_s$($r=zm, s=zn, r,s=1,\ldots,m_1,$)とおくと、半正定値行列$\mathrm{T} = \mathrm{A}\mathrm{A}^T$は、</p>
<p>$$
\mathrm{T} = \mathrm{A}\mathrm{A}^t = 
\left(
\begin{array}{ccc}
a_1 \\<br>
\vdots \\<br>
a_{m1} 
\end{array}
\right)
\left(
\begin{array}{ccc}
a_1 &amp;
\ldots &amp;
a_{m1} 
\end{array}
\right)=
\left(
\begin{array}{ccc}
1 &amp; a_1a_2 &amp; \cdots &amp; a_1a_{m1} \\<br>
a_2a_1 &amp; 1 &amp; \cdots &amp; a_2a_{m1}  \\<br>
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br>
a_{m1}a_1 &amp; a_{m1}a_{m2} &amp; \cdots &amp; 1
\end{array}
\right) \tag{18}
$$</p>
<p>と、$\tau_{r,s}$を$(r,s)$成分にもつ対角成分が1の行列(positive definite matrix with unit diagonal elements)になります。</p>
<p>以降、この性質をもつ行列を<strong>PDUDE</strong>と書きます。</p>
<p>以上のことから、</p>
<p>$$
\mathrm{T}(r,s)k_{\phi}(\mathrm{x}_m,\mathrm{x}_n)=cor(\epsilon(\mathrm{w}_m),\epsilon(\mathrm{w}_n)) \tag{19}
$$</p>
<p>は、質的変数と量的変数の影響を考慮することのできる相関関数ととらえることができます。</p>
<h5 id="質的変数が2つ以上の場合">質的変数が2つ以上の場合</h5>
<p>一般的なケースとして、$J$個の質的変数$\mathrm{z}=(z_1,\ldots,z_J)^T$の場合を考えます。</p>
<p>ここで、$z_j(j=1,\ldots,J)$は$1,\ldots,m_j$の値をとるものとします。すると、式$(19)$の拡張により、$\epsilon(\mathrm{w})$の相関は関数は以下のようになります。</p>
<p>$$
\prod_{j=1}^{J}\left( \tau_{z_{j,r,s}}k_{\phi}(\mathrm{x}_m,\mathrm{x}_n) \right)=cor(\epsilon(\mathrm{w}_m),\epsilon{\mathrm{w}_n}) \tag{20}
$$</p>
<p>$\tau_{j,r,s}$は、$J$番目のPDUDE$\mathrm{T}_j$の$(r,s)$成分です。</p>
<p>特に、$k_{\phi}(\mathrm{x}_m,\mathrm{x}_n)$に式$(10)$のガウスカーネルをもちいた場合、$(20)$式は以下のようになります。</p>
<p>$$
\prod_{j=1}^{J}\left( \tau_{z_{j,r,s}}k_{\phi}(\mathrm{x}_m,\mathrm{x}_n) \right)=
\prod_{j=1}^{J}\left(\tau_{z_{j,r,s}}exp\left(-\cfrac{1}{2\rho^2}\sum_{i=1}^{I}(x_{im},x_{in})^2\right)\right)=
\left(\prod_{j=1}^{J}\tau_{z_{j,r,s}}\right)exp\left(-\cfrac{1}{2\rho^2}\sum_{i=1}^{I}(x_{im}-x_{in})^2\right) \tag{21}
$$</p>
<p>式$(21)$は、量的変数の影響は$exp\left(-\cfrac{1}{2\rho^2}\sum_{i=1}^{I}(x_{im}-x_{in})^2\right)$で考慮し、質的変数の影響はそれとは独立に$\left(\prod_{j=1}^{J}\tau_{z_{j,r,s}}\right)$で考慮する、という構造を持っています。パラメータ$\tau_{z_{j,r,s}}$は、質的変数$z_j$について$r=z_jm$をとる入力$\mathrm{w}_m$と、$s=z_jn$をとる入力$\mathrm{w}_n$の、$z_j$のみによる共通性(相関)への影響を反映する役割を担っています。なお、式$(21)$ではshapeパラメータ$\alpha^2$を考慮していませんが、これは$(2)$式でshapeパラメータの役割を$\sigma$が受け持っている為です。</p>
<p>モデリングにおいては、$\tau_{z_{j,r,s}}$は正の値をとるように設定し、任意の2点の入力が無相関もしくは正の相関のみをとるようにします。</p>
<h5 id="制約のあるpdude">制約のあるPDUDE</h5>
<p>前節では、PDUDEについて制約を設けない相関行列を用いていました。柔軟なモデリングにおいてはこれで問題ないのですが、質的変数が順序尺度であったり、カテゴリカルな変数であったりするということがあらかじめ自明な場合は、PDUDEに制約を持たせることで、その情報をモデルに反映させることができます。ここでは、sesがカテゴリカルな変数であることから、質的変数がカテゴリカルな場合にPDUDEに設ける制約について説明します。</p>
<p>結論からですが、$m$個の値をとる$z$がカテゴリカルな場合、下記の$\tau_{r,s}$を$(r,s)$成分に持つ等方性を持った$m×m$相関行列$\mathrm{T}$が用いられます。</p>
<p>$$
\tau_{r,s}= \begin{cases}
c~~~(0&lt;c&lt;1)~~~\mathrm{if}~r\neq{s}\\<br>
1~~~\mathrm{if}~r=s
\end{cases}　\tag{22}
$$</p>
<p>$r=s$のとき、入力間の相関は$z$に関しては1、$r\neq{s}$のとき、c(一定)にする、ということです。
$\mathrm{T}$は以下のように分解できます。</p>
<p>$$
\mathrm{T}=
(1-c)
\left(
\begin{array}{ccc}
1 &amp; 0 &amp; \cdots &amp; 0 \\<br>
0 &amp; 1 &amp; \cdots &amp; 0  \\<br>
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br>
0 &amp; 0 &amp; \cdots &amp; 1
\end{array}
\right)+
c
\left(
\begin{array}{ccc}
1 \\<br>
1  \\<br>
\vdots \\<br>
1
\end{array}
\right)
\left(
\begin{array}{ccc}
1&amp;1&amp;\cdots,1 \\<br>
\end{array}
\right) \tag{23}
$$
このとき、任意の$m×1$ベクトルaについて、
$$
a^T\mathrm{T}a=(1-c)a-Ta+c(a-T1)^2　&gt;0 \tag{24}
$$
だから、$\mathrm{T}$は正定値行列なので、PDUDEです。$\mathrm{T}$が正定値行列であることは結構重要で、各要素の値を出力するカーネル関数$k(\mathrm{z}_m,\mathrm{z}_n)$が何かしらの特徴ベクトル空間の内積を表現するために必要な条件です。このあたりの詳細は<a href="https://mathwords.net/gramgyoretu">こちら</a>などを見てください。また上記$\mathrm{}T$の各要素を出力する関数は<strong>isotropic correlation function</strong>と呼ばれ、isotropic correlation functionによる出力を要素に持つ行列は<strong>compound symmetric correlation matrix</strong>と呼ばれています。</p>
<p>上記の$\mathrm{T}$をPDUDEに用いる場合、式$(21)$は下記のように変形できます。</p>
<p>$$
\left(\prod_{j=1}^{J}\tau_{z_{j,r,s}}\right)exp\left(-\cfrac{1}{2\rho^2}\sum_{i=1}^{I}(x_{im}-x_{in})^2\right)=
\prod_{j=1}^{J}exp\left(-ln\left(\cfrac{1}{c}\right)I[r\neq{s}]\right)exp\left(-\cfrac{1}{2\rho^2}\sum_{i=1}^{I}(x_{im}-x_{in})^2\right)=
exp\left(-\cfrac{1}{2\rho^2}\sum_{i=1}^{I}(x_{im}-x_{in})^2-\sum_{j=1}^{J}ln\left(\cfrac{1}{c}\right)I[r\neq{s}]\right) \tag{25}
$$</p>
<p>ここで、$I[r\neq{s}]$は下記の関数になります。</p>
<p>$$
I[r\neq{s}]=\begin{cases}
1~~~\mathrm{if}~r\neq{s}\\<br>
0~~~\mathrm{if}~r=s
\end{cases}　\tag{26}
$$</p>
<p>式$(25)$最後の項は、対数をとると</p>
<p>$$
log\left(exp\left(-\cfrac{1}{2\rho^2}\sum_{i=1}^{I}(x_{im}-x_{in})^2-\sum_{j=1}^{J}ln\left(\cfrac{1}{c}\right)I[r\neq{s}]\right)\right)=
-\cfrac{1}{2\rho^2}\sum_{i=1}^{I}(x_{im}-x_{in})^2-\sum_{j=1}^{J}ln\left(\cfrac{1}{c}\right)I[r\neq{s}]\tag{27}
$$</p>
<p>となります。よって、対数スケールにおいて、量的変数についてはL2距離を、質的変数については0~1の値をとる距離を使用していることが分かります。</p>
<p>以上、カテゴリカルな変数を用いたカーネルについて説明しました。参考文献は<a href="http://www2.isye.gatech.edu/~jeffwu/publications/gaussian_procmodels.pdf">こちら</a>になります。制約のあるPDUDEについては、今回紹介したもののほかにも順序尺度に対するPDUDE,グループ相関に対するPDUDE等紹介されています。</p>
<h1 id="多項ロジスティックガウス過程モデル">多項ロジスティック・ガウス過程モデル</h1>
<p>前置きが長くなってしまいましたが、ガウス過程・ロジスティック回帰モデルを実装します。</p>
<h5 id="モデルの確認">モデルの確認</h5>
<p>出力を$(N×1)$ベクトル$\mathrm{y}=(y_1,\ldots,y_N)^T$、入力を$(N×(I+J))$行列$\mathrm{W}=(\mathrm{w}_1,\ldots, \mathrm{w}_N)^T$とします。</p>
<p>$$
\mathrm{w}_n = (\mathrm{x}_n^T,\mathrm{z}_n^T) \tag{28}
$$</p>
<p>$\mathrm{x}_n$、$\mathrm{z}_n$はそれぞれ要素数$I$の質的変数、要素数$J$の質的変数です。</p>
<p>$$
\left(x_{n1},\ldots,x_{nI}\right)^T=\mathrm{x}_{n}
$$</p>
<p>$$
\left(z_{n1},\ldots,w_{nJ}\right)^T=\mathrm{w}_{n}
$$</p>
<p>$\mathrm{y}$が$K$個の値をとるとき、$\mathrm{y}$が各値をとる確率を表す$(N×K)$行列$\mathrm{\Theta}=(\mathrm{\theta}_1, \ldots, \mathrm{\theta}_K)$を導入します。</p>
<p>ここで$(\theta_{k1},\ldots,\theta_{kN})^T=\mathrm{\theta}_k~~~(k=1,\ldots,K)$です。</p>
<p>$$
\mathrm{y} \sim Categorical(\Theta)\tag{29}
$$</p>
<p>$\Theta$は1行の要素の和をとると1になるので、各要素が$(-\infty,\infty)$の値をとる$(N×K)$行列$\mathrm{P}=(\mathrm{p}_1,\ldots,\mathrm{p}_K)$を導入し、下記のようにします。</p>
<p>ここで、$(p_{k1},\ldots,p_{kN})^T=\mathrm{p}_k~~(k=1,\ldots,K)$です。</p>
<p>$$
(\theta_{1n},\ldots,\theta_{Kn})^T = softmax(\mathrm{P}_n^T)~~~(n=1,\ldots,N) \tag{30} 
$$</p>
<p>ここで$(p_{1n},p_{2n},\ldots,p_{Kn})=\mathrm{P}_n$です。</p>
<p>ガウス過程で推定する対象は、上記モデルの$\mathrm{P}$になります。多項ロジスティック回帰では、出力が任意の特定の値をとる確率を固定する必要があったので、それを踏まえて下記のようにおきます。</p>
<p>$$
\mathrm{p}_k = \begin{cases}
0~~~\mathrm{if}~k=1\\<br>
\mathrm{L}\eta_k~~~\mathrm{if}~k=2,\ldots,K
\end{cases}\tag{31}
$$</p>
<p>$$
\mathrm{LL^T} = \mathrm{K}\tag{32}
$$</p>
<p>$$
\eta_k=(\eta_{k1},\ldots,\eta_{kN}) \sim Normal(0,1) ~~(k=2,\ldots,K)\tag{33}
$$</p>
<p>ここで、$\mathrm{K}$は$(N×N)$のカーネル行列で、$(m,n)$成分をカーネル関数$k(\mathrm{w}_m,\mathrm{w}_n)$とします。カーネル関数の設定を工夫することで、線形モデルから得られた知見やデータの特徴(カテゴリカル変数が含まれる点)などをモデルに反映させていきます。</p>
<h5 id="カーネル関数の設定方法">カーネル関数の設定方法</h5>
<p>本記事では2つのカーネル関数を設定し、2段階に分けて多項ロジスティック回帰・ガウス過程モデルを実行します。</p>
<p><strong>モデル① 線形カーネル + isotropic correlationカーネル</strong></p>
<p>量的変数writeの影響は線形カーネルで取り入れ、カテゴリカル変数sesの影響をPDUDEである$\mathrm{T}$で考慮する、という構造です。$\mathrm{T}$の各要素はisotropic correlation functionで出力されます。
<!-- raw HTML omitted -->  <br>
このカーネル関数をロジスティック回帰・ガウス過程モデルに用いることで、これまでの記事<a href="https://rmorita-stat.github.io/my-page/post/multinom/multinom/">(1)</a>、<a href="https://rmorita-stat.github.io/my-page/post/multinom-rstan/multinom-rstan/">(2)</a>で得られた結果と同じ結果が得られることを確認し、モデルの妥当性チェックを行います。</p>
<p>カーネル関数は下になります。</p>
<p>$$
\theta_1\sum_{i=1}^{I}x_{mi}x_{ni} + \theta_2\sum_{j=1}^J\mathrm{T}(z_{nj},z_{mj}) = k(\mathrm{w}_m,\mathrm{w}_n) \tag{34}
$$</p>
<p>ここで、</p>
<p>$$
\mathrm{T} = \left(
\begin{array}{ccc}
1 &amp; c &amp; \cdots &amp; c \\<br>
c &amp; 1 &amp; \cdots &amp; c  \\<br>
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br>
c &amp; c &amp; \cdots &amp; 1
\end{array}
\right)\tag{35}
$$</p>
<p>は、式$(22)$、$(23)$あたりで説明したものす。</p>
<p>ハイパーパラメータは$\theta_1$、$\theta_2$、$c$の3つです。</p>
<p><strong>モデル② 線形カーネル + (ガウスカーネル × isotropic correlationカーネル)</strong></p>
<p>量的変数writeの影響をモデル①にように線形的に把握するだけでなく、線形モデルでは誤差として扱われてしまう要素まで結果に取り入れるため、線形カーネルとガウスカーネルを組み合わせて量的変数の影響を予測します。質的変数の影響はモデル①と同様にisotropic correlationカーネルで予測します。</p>
<p>カーネル関数は下になります。</p>
<p>$$
\theta_1\sum_{i=1}^{I}x_{mi}x_{ni} +\theta_2exp\left(-\cfrac{1}{2\rho^2}\sum_{i=1}^{I}(x_{im}-x_{in})^2-\sum_{j=1}^{J}ln\left(\cfrac{1}{c}\right)I[r\neq{s}]\right)　\tag{36}
$$</p>
<p>ハイパーパラメータは$\theta_1$,$\theta2$,$\rho$,$c$の4つです。</p>
<p>なお、$(36)$式第一項の線形カーネルの部分は、ガウス過程として扱わないで式$(1&rsquo;)$の$\beta\mathrm{f}(\mathrm{w})$で扱うこともできるのですが、今回はフルガウス過程で実行してみます。</p>
<h5 id="モデルの実装と結果の確認">モデル①の実装と結果の確認</h5>
<p>モデル①を実行するStanコードを以下のようになります。</p>
<pre><code>//model2
data{
  int&lt;lower=2&gt; K; //カテゴリ数
  int&lt;lower=1&gt; N1; //データ数
  int N2; //予測入力数
  int&lt;lower=1&gt; I; //量的変数の数
  int&lt;lower=1&gt; J; //質的変数の数
  int M[J]; //各質的変数のカテゴリ数
  int&lt;lower=1, upper=K&gt; y[N1]; //出力ラベル
  int&lt;lower=1, upper=max(M)&gt; x1_j[N1,J]; //入力(質的変数)
  vector[I] x1_i[N1];//入力(量的変数)
  int&lt;lower=1, upper=max(M)&gt; x2_j[N2,J]; //予測したい点(質的変数)
  vector[I] x2_i[N2]; //予測したい点(量的変数)
}

transformed data{
  real delta = 1e-9;
  int&lt;lower=1&gt; N = N1 + N2;
  int&lt;lower=1, upper=max(M)&gt; x_j[N,J]; //入力(質的変数)と予測入力(質的変数)を縦に繋げたもの
  vector[I] x_i[N]; //入力(量的変数)と予測入力(量的変数)を縦に繋げたもの

  //以下でx1_iとx2_i,x1_jとx2_jを縦に結合し、新たな行列x_i,x_jをそれぞれ作成する
  for(n in 1:N1){
    for(j in 1:J){
      x_j[n,j] = x1_j[n,j];
    }
  }
  for(n in 1:N2){
    for(j in 1:J){
      x_j[(N1+n),j] = x2_j[n,j];
    }
  }
  for(n in 1:N1) x_i[n] = x1_i[n];  
  for(n in 1:N2) x_i[N1+n] = x2_i[n];
  

}

parameters{
  vector[N] eta[(K-1)]; //潜在変数
  vector&lt;lower=0&gt;[2] theta;
  vector&lt;lower=0, upper=1&gt;[J] C;
}

transformed parameters{
  vector[N] f[(K-1)];
  row_vector[K] p[N]; //ソフトマックス関数に投げる値
  
  //この中でカーネル関数を定義、ガウス過程の関数fを作成
  {
    matrix[N,N] L_K[(K-1)];
    matrix[N,N]linear_kernel;
    matrix[N,N]isotropic_correlation_kernel = rep_matrix(0,N,N);//初期値を0に設定。
    matrix[N,N] kernel_matrix[(K-1)];
    matrix[max(M),max(M)] PDUDE[J];
    
    //compound symmetric correlation matrix を作成
    for(j in 1:J){
      for(n in 1:(max(M)-1)){
      PDUDE[j][n,n] = 1;
      for(m in (n+1):max(M)){
        PDUDE[j][n,m] = C[j];
        PDUDE[j][m,n] = PDUDE[j][n,m];
        }
      }
      PDUDE[j][max(M),max(M)] = 1;
    }
    
    
    //isotropic_correlation_kernelを生成
    for(j in 1:J){
      for(n in 1:N){
        for(m in 1:N){
          isotropic_correlation_kernel[n,m] += PDUDE[j][x_j[n,j],x_j[m,j]];
        }
      }
    }
    
    //linear_kernel_matrixを生成。切片項はisotropic_correlation_kernelで捉えることにした
    for(n in 1:N){
      for(m in1:N){
        linear_kernel[n,m] = dot_product(x_i[n],x_i[m]);
      }
    }
    //kernel_matrixを作成
    for(k in 1:(K-1)){
      kernel_matrix[k] = theta[1] * linear_kernel + theta[2] * isotropic_correlation_kernel;
      for(n in 1:N){
        kernel_matrix[k][n,n] += delta;
      }
    }
    for(k in 1:(K-1)){
      L_K[k] = cholesky_decompose(kernel_matrix[k]);
      f[k] = L_K[k] * eta[k];
    }
  }
  
  // pの作成 1列目は0に固定 2列目以降に独立のガウス過程を指定
  for(n in 1:N){
     p[n,1] = 0;
     for(k in 1:(K-1)){
        p[n,(k+1)] = f[k,n];
     }
  }
}

model{
  //事前分布
  theta ~ student_t(4,0,2);
  C ~ normal(0,0.1);
  
  //モデル部分
  for(k in 1:(K-1)){
    for(n in 1:N){
      eta[k,n] ~ std_normal();
    }
  }
  for(n in 1:N1){
    y[n] ~ categorical_logit(p[n]');
  }
  
}

generated quantities{
  vector[K] pred[N2];
  for(n in 1:N2){
    pred[n] = softmax(p[(N1+n)]');
  }
}
</code></pre>
<p>入力データを命令する<code>data{}</code>ブロックでは、入力データ(<code>x1_i</code>、<code>x1_j</code>)を量的変数・質的変数で分けて指定しています。また、予測したい入力点(<code>x2_i</code>、<code>x2_j</code>)も指定しています。</p>
<p><code>transformes data{}</code>ブロックでは、式$(12)$の計算を実行するため、入力(実現値)と予測したい入力点のデータを量的変数・質的変数毎に縦に繋げています。</p>
<p><code>transformed parameter{}</code>ブロックでは、まず、式$(35)$の行列を<code>PDUDE</code>という呼称で作成しています。次に、<code>issotropic_correlation_kernel</code>として、<code>PDUDE</code>を参照しながらcompound symmetric correlation matrixを作成しています。
<!-- raw HTML omitted -->  <br>
また、<code>linear_kernel</code>として線形カーネルの要素を作成しています。線形カーネルはStanに実行された<code>dot_product()</code>が便利です。
<!-- raw HTML omitted -->  <br>
さらに、<code>kernel_matrix</code>として式$(34)$のカーネル関数を各要素に持つカーネル関数を作成します。
このように、線形カーネル・isotropic_correlation_kernelカーネルを作成し、それらを結合させてカーネル行列を作成し、最後に既述の方法でガウス過程に従う$\mathrm{f}$の定義までを<code>transformed parameter{}</code>で命令しています。</p>
<p><code>model{}</code>ブロックでは、まず事前分布を式$(34)$の$\theta_1$、$\theta_2$、式$(35)$の$c$で、適当に指定しています。
また、式$(9)$下段の設定を<code>eta ~ std_normal()</code>として指定し、多項ロジスティック回帰モデルの式$(29)$、式$(30)$を<code>y[n] ~ categorical_logit(p[n]');</code>としています。</p>
<p><code>generated quantities{}</code>ブロックでは、<code>model</code>ブロックでは作成しなかった入力点<code>x2_i</code>、<code>x2_j</code>における予測値を<code>pred</code>として指定しています。</p>
<p>このStanファイルを実行するコードは以下になります。</p>
<pre><code># progの3要素を数字に置き換えるための表を作成
progid &lt;- c(1,2,3)
names(progid) &lt;- c(&quot;academic&quot;,&quot;general&quot;,&quot;vocation&quot;)
# sesの3要素を数字に置き換えるための表を作成
sesid &lt;- c(1,2,3)
names(sesid) &lt;- c(&quot;low&quot;,&quot;middle&quot;,&quot;high&quot;)


library(makedummies)
library(tidyr)
library(dplyr)

d2 &lt;- ml %&gt;% mutate(sesid = sesid[paste(ses)]) %&gt;% 
  mutate(progid = progid[paste(prog)]) %&gt;% mutate(write=(write-mean(write))/sd(write)) %&gt;% select(c(sesid, write, progid))

#x1:入力点
x1_j &lt;- data.frame(d2[,-c(2,3)]) # 入力(質的変数)
x1_i &lt;- data.frame(d2[,-c(1,3)]) # 入力(量的変数)
N1 &lt;- nrow(x1_i)
#x2:予測入力点
x2_j &lt;- data.frame(sesid = c(rep(1,41),rep(2,41),rep(3,41))) # 予測入力点(質的変数)
x2_i &lt;- data.frame(write = (rep(c(30:70),3)-mean(ml$write))/sd(ml$write)) # 予測入力点(量的変数)
N2 &lt;- nrow(x2_i)

#y:出力点
y &lt;- d2[,3]
K &lt;- 3
J &lt;- 1 #質的変数の数
I &lt;- 1 #量的変数の数
M &lt;- c(3) #各量的変数のカテゴリ数
dim(M) &lt;- 1

data &lt;- list(x1_i=x1_i,x1_j=x1_j,x2_i = x2_i,x2_j=x2_j,N1=N1,N2=N2,K=K,y=y,J=J,I=I, M=M)

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
fit_cate1 &lt;- stan(file=&quot;model2.stan&quot;, data=data, pars = c(&quot;pred&quot;,&quot;theta&quot;,&quot;C&quot;), warmup = 400, iter = 1500, chains=4, seed=1234)
</code></pre>
<p>計算にかかった時間はsurface laptop2 Corei5-8250Uで約3000秒でした。
ガウス過程は逆行列の計算などを含むので、工夫をしないとどうしても計算に時間がかかってしまうようです。</p>
<p>ハイパーパラメータの事後分布は以下のようになりました。</p>
<p><img src="/my-page/post/multinom-gp/fit_cate1_density.png" alt=""></p>
<p>予測結果は以下のようになりました。描画の為のコードは<a href="https://rmorita-stat.github.io/my-page/post/multinom-rstan/multinom-rstan/">以前の記事</a>を参照してください。</p>
<p><img src="/my-page/post/multinom-gp/fit_cate1_res.png" alt=""></p>
<p>rのパッケージnnetのmultinom()関数を使ったときや、Rstanで線形予測子Versionの多項ロジスティック回帰を実行したときの結果と同じ結果が得られていることが分かります。</p>
<p>以上、sotropic correlationカーネルを含んだモデル全体が正常に動くことを確認し、次の分析に進みます。</p>
<h5 id="モデルの実行と結果の確認">モデル②の実行と結果の確認</h5>
<p>モデル②を実行するStanコードを以下のようになります。</p>
<pre><code>//model3
data{
  int&lt;lower=2&gt; K; //カテゴリ数
  int&lt;lower=1&gt; N1; //入力データの数
  int N2; //予測したい入力点の数
  int&lt;lower=1&gt; I; //量的変数の数
  int&lt;lower=1&gt; J; //質的変数の数
  int M[J]; //各質的変数のカテゴリ数
  int&lt;lower=1, upper=K&gt; y[N1]; //出力ラベル
  int&lt;lower=1, upper=max(M)&gt; x1_j[N1,J]; //入力(質的変数)
  vector[I] x1_i[N1];//入力(量的変数)
  int&lt;lower=1, upper=max(M)&gt; x2_j[N2,J]; //予測した入力点(質的変数)
  vector[I] x2_i[N2]; //予測したい入力点(量的変数)
}

transformed data{
  real delta = 1e-9;
  int&lt;lower=1&gt; N = N1 + N2;
  int&lt;lower=1, upper=max(M)&gt; x_j[N,J]; //入力(質的変数)と予測入力(質的変数)を縦に繋げたもの
  vector[I] x_i[N]; //入力(量的変数)と予測入力(量的変数)を縦に繋げたもの
  matrix[max(M),max(M)] inv_I; //対角成分が0、それ以外が1の行列を生成

  //以下でx1_iとx2_i,x1_jとx2_jを縦に結合し、新たな行列x_i,x_jをそれぞれ作成する
  for(n in 1:N1){
    for(j in 1:J){
      x_j[n,j] = x1_j[n,j];
    }
  }
  for(n in 1:N2){
    for(j in 1:J){
      x_j[(N1+n),j] = x2_j[n,j];
    }
  }
  for(n in 1:N1) x_i[n] = x1_i[n];  
  for(n in 1:N2) x_i[N1+n] = x2_i[n];
  
  //inv_Iの作成
  for(n in 1:(max(M)-1)){
    inv_I[n,n] = 0;
    for(m in (n+1):max(M)){
      inv_I[n,m] = 1;
      inv_I[m,n] = 1;
    }
  }
  inv_I[max(M),max(M)] = 0;

}

parameters{
  vector[N] eta[(K-1)]; //潜在変数
  vector&lt;lower=1&gt;[2] theta;線形カーネルと「ガウスカーネル・isotropic correlationカーネル」の重みを決定するハイパーパラメータ
  vector&lt;lower=0, upper=1&gt;[J] C; //isotropic correlationカーネルのハイパーパラメータ
  real&lt;lower=0, upper=0.9&gt; rho; //ガウスカーネルのshapeパラメータ
}

transformed parameters{
  vector[N] f[(K-1)];
  row_vector[K] p[N]; //ソフトマックス関数に投げる値
  
  
  //この中でカーネル関数を定義、ガウス過程に従う関数fを作成
  {
    matrix[N,N] L_K[(K-1)]; //カーネル行列をコレスキー分解した行列L。（出力がとる値）-1の数だけ準備する
    matrix[N,N]linear_kernel; //線形カーネル
    matrix[N,N]distance_L2_01_kernel; //。ガウスカーネル×isotropic correlation カーネル。logスケールでL2距離と0-1距離の成分を含んでいる
    matrix[N,N] kernel_matrix[(K-1)]; //最終的に作成するカーネル関数
    
    
    //linear_kernelを生成
    for(n in 1:N){
      for(m in1:N){
        linear_kernel[n,m] = dot_product(x_i[n],x_i[m]);
      }
    }
    
    //distanceL2_01_kernelを生成
    {
      matrix[N,N] distance_L2; //L2距離
      matrix[N,N] distance_01 = rep_matrix(0,N,N); //inv_Iに依存した0-1距離
      
      for(n in 1:N){
        for(m in 1:N){
          distance_L2[n,m] = dot_self((x_i[n]-x_i[m]) ./ rho);
        }
      }
      
      for(j in 1:J){
        for(n in 1:N){
          for(m in 1:N){
            distance_01[n,m] += log(1/C[j])*inv_I[x_j[n,j],x_j[m,j]];
          }
        }
      }
      
      for(n in 1:N){
        for(m in 1:N){
          distance_L2_01_kernel[n,m] = exp(-0.5 * distance_L2[n,m] -distance_01[n,m]);
        }
      }
    }
    
    
    //kernel_matrixを作成
    for(k in 1:(K-1)){
      kernel_matrix[k] = theta[1] * linear_kernel + theta[2] * distance_L2_01_kernel;
      for(n in 1:N){
        kernel_matrix[k][n,n] += delta;
      }
    }
    
    for(k in 1:(K-1)){
      L_K[k] = cholesky_decompose(kernel_matrix[k]);
      f[k] = L_K[k] * eta[k];
    }
  }
  
  // pの作成 1列目は0に固定 2列目以降に独立のガウス過程を指定
  for(n in 1:N){
     p[n,1] = 0;
     for(k in 1:(K-1)){
        p[n,(k+1)] = f[k,n];
     }
  }
}

model{
  //事前分布
  theta ~ student_t(4,1,2);
  rho ~ inv_gamma(0.1, 0.1);
  C ~ normal(0, 0.1);
  
  //モデル部分
  for(k in 1:(K-1)){
    for(n in 1:N){
      eta[k,n] ~ std_normal();
    }
  }
  for(n in 1:N1){
    y[n] ~ categorical_logit(p[n]');
  }
  
}

generated quantities{
  vector[K] pred[N2];
  for(n in 1:N2){
    pred[n] = softmax(p[(N1+n)]');
  }
}
</code></pre>
<p>モデル①との違いは、<code>transformed parameters{}</code>ブロックにおけるカーネル行列作成の部分と、<code>model{}</code>ブロックにおける事前分布設定の部分になります。</p>
<p><code>transformed parameters{}</code>ブロックでは、<code>linear_kernel</code>として式$(36)$第1項の部分（線形カーネル）の$\theta_1$を除いた部分を、<code>distance_L2_01_kernel</code>として式$(36)$第2項の$\theta_2$を除いた部分を作成しています。また<code>distance_L2_01_kernel</code>は、<code>distance_L2</code>として$\sum_{i=1}^{I}(x_{im}-x_{in})^2$を、<code>distance_01_kernel</code>として$\sum_{j=1}^{J}ln\left(\cfrac{1}{c}\right)I[r\neq{s}]$をそれぞれ作成し、それらを式$(36)$に従って合成することで作成しています。</p>
<p>ハイパーパラメータの事前分布の設定はかなり苦戦しました。最終的に以下の事前分布を採用しています。</p>
<p>$$
\theta_1 \sim Student_t(4,1,2) ~~~\theta_1 \geq 1\\<br>
\theta_2 \sim Student_t(4,1,2) ~~~\theta_2 \geq 1\\<br>
\rho \sim invGamma(0.1,0.1) \\<br>
c \sim Normal(0,0.1)~~~0 \leq c \leq 0.9 
$$</p>
<p>$\theta_1$、$\theta_2$については弱情報事前分布として、自由度4、期待値1、スケールパラメータ1の半t分布を採用しています。
<!-- raw HTML omitted -->  <br>
$\rho$については、Stanマニュアルを参照して、0付近の値を避けることができつつも小さな値にとがった山を持ち、かつ十分大きな値にも対応可能な逆ガンマ分布を採用しています。
<!-- raw HTML omitted -->  <br>
$c$については、0付近の値をとることが想定されるため、期待値0、標準偏差0.1の切断正規分布を採用しています。</p>
<p>様々な事前分布を試していたのですが、ガウスカーネルにおいて、shapeパラメータ$\rho$とrateパラメータ$\theta$の比が重要なようで、shapeパラメータに比べてrateパラメータが十分大きいと、出力値の変化の傾きが小さくなり、式$(1)$の誤差項を捉えてくれなくなってしまいます。この点についてはStanマニュアルにも以下の記載があります。</p>
<blockquote>
<p>Perhaps most importantly, the parameter $\rho$ and $\alpha$ are weakly identified Zhang(2004). The ratio of the rwo parameters is well-identified&hellip;</p>
</blockquote>
<p>今回の場合、$\rho$に値の上限を設定しないとどうしても$\rho$が大きくなってしまい、予測結果もモデル式①と変わらなくなってしまいました。しかしそれはこのモデルの意図した結果ではありません。</p>
<p>以前の<a href="https://rmorita-stat.github.io/my-page/post/gaussianprocessori/gaussianprocessori/">ガウス過程のシミュレーション結果</a>を見ると、標準化されたデータの場合、shapeパラメータ、rateパラメータともに1前後でちょうどよいガウス過程からの出力が得られそうであることが確認できます。よって、今回はrateパラメータに0.9の上限を設け、shapeパラメータも下限値1を設定することで、少し変化の傾きが急な出力を得られるように強要することにします。</p>
<p>上記のStanファイルを実行するコードは以下になります。</p>
<pre><code>data &lt;- list(x1_i=x1_i,x1_j=x1_j,x2_i = x2_i,x2_j=x2_j,N1=N1,N2=N2,K=K,y=y,J=J,I=I, M=M)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
fit_cate2 &lt;- stan(file=&quot;model3.stan&quot;, data=data, pars = c(&quot;pred&quot;,&quot;rho&quot;,&quot;theta&quot;,&quot;C&quot;), warmup = 400, iter = 1500, chains=4, seed=1234)
</code></pre>
<p>計算にかかったた時間は確かおよそ9800秒でした。2時間以上かかっていますね。PCもうなり声をあげていたので冷却対策など必要かもしれません。</p>
<p>ハイパーパラメータの事後分布は以下のようになります。</p>
<pre><code>posterior_fit0_2 &lt;- rstan::extract(fit_cate2)
library(bayesplot)
plot_title &lt;- ggtitle(&quot;Posterior distribution of hyper parameters&quot;, &quot;with medians and 95% intervals&quot;)
p1 &lt;- mcmc_areas(as.matrix(fit_cate2),
                regex_pars = c(&quot;theta&quot;),prob=0.95, area_method = &quot;equal height&quot;) +
  scale_y_discrete(labels=c(&quot;theta1&quot;,&quot;theta2&quot;)) + theme_bw(base_size=12)
p2 &lt;- mcmc_areas(as.matrix(fit_cate2),
                regex_pars = c(&quot;C&quot;,&quot;rho&quot;),prob=0.95, area_method = &quot;equal height&quot;) +
  scale_y_discrete(labels=c(&quot;c&quot;,&quot;rho&quot;)) +
   theme_bw(base_size=12) 
library(gridExtra)

g1 &lt;- ggplot_gtable(ggplot_build(p1))
g2 &lt;- ggplot_gtable(ggplot_build(p2))
Width &lt;- unit.pmax(g1$widths, g2$widths)
Height &lt;- unit.pmin(g1$heights, g2$heights)
g1$widths &lt;- Width
g2$widths &lt;- Width
g1$heights &lt;- Height
g2$heights &lt;- Height
p &lt;- gridExtra::grid.arrange(g1,g2,ncol=1, top=&quot;Posterior distribution of hyper parameters (with medians and 95% intervals)&quot;)
</code></pre>
<p><img src="/my-page/post/multinom-gp/fit_cate2_density.png" alt=""></p>
<p>$\rho$が頑張って大きな値をとろうとしている様子が見えますが、そこは抑えてもらっています。何だかかわいそう&hellip;</p>
<p>最後に、予測結果を描画します。</p>
<p><img src="/my-page/post/multinom-gp/fit_cate2_res.png" alt=""></p>
<p>線形モデルでは読み取ることが出来なかった傾向がうまく捉えられています。例えば、読み書きの能力が50前後のses=lowの生徒はgeneralの授業をとる傾向にあること、読み書きの能力が45前後のses=highの生徒はacademicの授業をとる確率とvocationの授業をとる確率が同程度である様子などが確認できます。</p>
<p><strong>It&rsquo;s so brilliant</strong>&gt;🐢</p>
<p>このように、線形モデルでは誤差として結果に反映されなかった事象もうまくとらえることが出来るのがガウス過程の魅力です。非常に便利なモデルだと私は思っています。</p>
<h1 id="まとめ">まとめ</h1>
<p>今回はガウス過程を多項ロジスティック回帰に取り込んだモデルの実装を行いました。その過程で、ガウス過程潜在変数モデルやカテゴリカルデータを取り入れたガウス過程等、なかなか厄介なものも導入しています。</p>
<p>この記事で、以前に述べたガウス過程の活用方法の１つ目(下記)を紹介した形です。</p>
<ul>
<li>一般化線形モデルの線形予測子をガウス過程に置き換え、柔軟なモデルに豹変させる</li>
</ul>
<p>多項ロジスティック回帰の記事も3つ目になりましたが、これで最後になります。最近はアウトプットに力を注いでいて投稿頻度も多かったですが、今後暫くはインプットに集中したいと考えており、投稿しない月が続くかもしれません。</p>
<p>Enjoy Stan!</p>

        </article>
    </div>
    <div class="main-content__tags u-font">
        
        
        <span><a href="https://rmorita-stat.github.io/my-page/tags/%E5%9B%9E%E5%B8%B0">回帰</a></span>
        
        <span><a href="https://rmorita-stat.github.io/my-page/tags/%E3%82%AC%E3%82%A6%E3%82%B9%E9%81%8E%E7%A8%8B">ガウス過程</a></span>
        
        <span><a href="https://rmorita-stat.github.io/my-page/tags/rstan">rstan</a></span>
        
        <span><a href="https://rmorita-stat.github.io/my-page/tags/%E3%83%8E%E3%83%B3%E3%83%91%E3%83%A9%E3%83%A1%E3%83%88%E3%83%AA%E3%83%83%E3%82%AF%E3%83%99%E3%82%A4%E3%82%BA%E3%83%A2%E3%83%87%E3%83%AB">ノンパラメトリックベイズモデル</a></span>
        
        
    </div>
</div>
<div class="main-profile">
    <div class="main-profile__avatar">
        
    </div>
    <div class="main-profile__body">
        <div class="main-profile__author">
            
            <span> R.morita </span>
            
        </div>
        <div class="main-profile__description">
            
            <p> 洛中で6年間大学生活を過ごし、今は難波の地で働いています。統計、ロードバイク、古墳が好きです。 </p>
            
        </div>
    </div>
</div>
<div class="main-line"></div>
<div class="main-pn">
    
    <a class="previous" href="https://rmorita-stat.github.io/my-page/post/gaussianprocessori/gaussianprocessori/">
        <div class="pn-dec"></div>
        <div class="pn-els">
            <div class="pn-el__1"> 2020.06.14 00:00 </div>
            <div class="pn-el__2"> ガウス過程入門 </div>
        </div>
    </a>
    
    
</div>

<footer>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
    });
</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  "HTML-CSS": {
    availableFonts: ["TeX"]
  }
  });
</script>
</footer>

</div>
<div class="footer">
    <div class="copyright-wrap">
        <p class="copyright u-font">
            
            &#169;
            2020
            
            <a href="https://github.com/Rmorita-stat/doc" target="_blank">R.morita&#46;</a>
            Theme <a href="https://github.com/iCyris/hugo-theme-yuki" target="_blank">yuki</a>&#46;
            Powered by Hugo&#46;
            
            
        </p>
    </div>
</div>
</body>
<script src="https://rmorita-stat.github.io/my-page/js/page.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

