<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>
        
        SUCRE HECACHA
        
    </title>
    <meta name="viewport" id="viewport" content="width=device-width, initial-scale=1">
    <link rel='icon' type='image/x-icon' href="https://rmorita-stat.github.io/my-page/images/logo2.png" />
    <link rel="apple-touch-icon" href="https://rmorita-stat.github.io/my-page/images/logo2.png"><link rel="stylesheet" href="https://rmorita-stat.github.io/my-page/scss/style.css">
    
    <link rel="stylesheet" href="https://rmorita-stat.github.io/my-page/scss/monokai-sublime.min.css">
    <script src="https://rmorita-stat.github.io/my-page/js/highlight.min.js"></script>
    <link rel="stylesheet" href="https://rmorita-stat.github.io/scss/highlight.css">
    
    <link rel="stylesheet" href="https://rmorita-stat.github.io/scss/custom.css">
    
    
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'Your Google Analytics tracking id', 'auto');
        ga('send', 'pageview');
    </script>
    
    
    <meta name="generator" content="Hugo 0.71.0" /></head>


<body>
<div class="header">
    <div class="site-logo__wrap">
        <div id="site-button">
            <div></div>
        </div>
        
        <div class=' site-logo '>
            <a href="https://rmorita-stat.github.io/my-page/"><img src="https://rmorita-stat.github.io/my-page/images/logo.png" /></a>
        </div>
    </div>
    
<div class=' site-nav u-font ' id="nav-bar">
    
    <div class="site-nav__wrap">
        <a class="site-nav__el" href="/my-page/" >HOME</a>
    </div>
    
    <div class="site-nav__wrap">
        <a class="site-nav__el" href="/my-page/post" >BLOG</a>
    </div>
    
    <div class="site-nav__wrap">
        <a class="site-nav__el" href="/my-page/about" >ABOUT</a>
    </div>
    
    <div class="site-nav__wrap">
        <a class="site-nav__el" href="/my-page/tags/" >TAGS</a>
    </div>
    
</div>

</div>
<div class="main">

<div class="main-content">
    <div class="main-content__date">
        <h4 id="date"> 2020.07.12 00:00 </h4>
    </div>
    <div class="main-content__title">
        <h1 id="title">多項ロジスティック回帰・ガウス過程モデル</h1>
    </div>
    <div class="main-content__article">
        <article id="content">
            <h1 id="はじめに">はじめに</h1>
<p><strong>回帰</strong>に関する記事です。</p>
<p><a href="https://rmorita-stat.github.io/my-page/post/multinom/multinom/">以前の記事</a>では、対象が3つ以上のクラスに分類されるとき、それぞれのクラスに属する確率を予測するモデルである<strong>多項ロジスティック回帰</strong>について書きました。</p>
<p><a href="https://rmorita-stat.github.io/my-page/post/multinom-rstan/multinom-rstan/">次の記事</a>では、同様のモデルを、<strong>マルコフ連鎖モンテカルロ法</strong>(MCMC)を実行するための汎用ソフトである<strong>Stan</strong>を用いて実装し、<strong>ベイズ統計</strong>の立場から分析しました。MCMCおよびベイズ統計については<a href="https://rmorita-stat.github.io/my-page/post/bayesintroduction/bayesintroduction/">こちらの記事</a>で(走り書きで)書いています。</p>
<p>上で見た記事ではいずれも対象が各クラスに属する確率のオッズ比に着目し、オッズ比の線形性のみを考慮したモデルです。このように統計モデルでは変数の背後に何かしらの分布を想定したり、変数間に線形的な関係を想定したりすることでモデルを構築します。</p>
<p>一方で、特定の分布や関係を仮定せず、与えられたデータに柔軟に対応することのできるモデルも存在します。そのようなモデルの例として<a href="https://rmorita-stat.github.io/my-page/post/gaussianprocess/gaussianprocess/">こちらの記事</a>では<strong>ガウス過程</strong>回帰について取り上げました。</p>
<p>またガウス過程を様々なデータやモデルに応用できるよう、<a href="https://rmorita-stat.github.io/my-page/post/gaussianprocess2/gaussianprocess2/">前回の記事</a>では<strong>ガウス過程潜在変数モデル</strong>や<strong>カテゴリカルな変数に対応できるカーネル</strong>、<strong>予測分布の計算方法</strong>について説明しました。</p>
<p>今回の記事はこれらの記事の内容をフル活用し、多項ロジスティック回帰にガウス過程を適用したモデルを実装します。</p>
<p>最終目標は、対象が各クラスに属する確率の予測値を、データに柔軟にフィットさせることです。</p>
<p>本記事の構成は以下の通りとします。</p>
<!-- raw HTML omitted -->
<ul>
<li><a href="#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB">はじめに</a></li>
<li><a href="#%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E5%85%A5%E6%89%8B">データの入手</a></li>
<li><a href="#%E5%A4%9A%E9%A0%85%E3%83%AD%E3%82%B8%E3%82%B9%E3%83%86%E3%82%A3%E3%83%83%E3%82%AF%E3%83%BB%E3%82%AC%E3%82%A6%E3%82%B9%E9%81%8E%E7%A8%8B%E3%83%A2%E3%83%87%E3%83%AB">多項ロジスティック・ガウス過程モデル</a>
<ul>
<li><a href="#%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%A2%BA%E8%AA%8D">モデルの確認</a></li>
<li><a href="#%E3%82%AB%E3%83%BC%E3%83%8D%E3%83%AB%E9%96%A2%E6%95%B0%E3%81%AE%E8%A8%AD%E5%AE%9A%E6%96%B9%E6%B3%95">カーネル関数の設定方法</a></li>
<li><a href="#%E3%83%A2%E3%83%87%E3%83%AB%EF%BC%91%E3%81%AE%E5%AE%9F%E8%A3%85%E3%81%A8%E7%B5%90%E6%9E%9C%E3%81%AE%E7%A2%BA%E8%AA%8D">モデル１の実装と結果の確認</a></li>
<li><a href="#%E3%83%A2%E3%83%87%E3%83%AB%EF%BC%92%E3%81%AE%E5%AE%9F%E8%A3%85%E3%81%A8%E7%B5%90%E6%9E%9C%E3%81%AE%E7%A2%BA%E8%AA%8D">モデル２の実装と結果の確認</a></li>
</ul>
</li>
<li><a href="#%E3%81%BE%E3%81%A8%E3%82%81">まとめ</a></li>
</ul>
<!-- raw HTML omitted -->
<p>当初本記事と<a href="https://rmorita-stat.github.io/my-page/post/gaussianprocess2/gaussianprocess2/">前回の記事</a>はひとつの記事として公開していましたが、余りにも長すぎる記事だったので、二つに分けて公開しなおしました。</p>
<h1 id="データの入手">データの入手</h1>
<p><a href="https://rmorita-stat.github.io/my-page/post/multinom/multinom/">以前の記事</a>でRのnnetパッケージを使って多項ロジスティック回帰分析をしたときに用いたものと同じデータを用いますある学校の生徒の属性と各生徒が選択した授業に関するデータです。</p>
<pre><code>library(foreign)
ml &lt;- read.dta(&quot;https://stats.idre.ucla.edu/stat/data/hsbdemo.dta&quot;)
head(ml)

##    id female    ses schtyp     prog read write math science socst       honors
## 1  45 female    low public vocation   34    35   41      29    26 not enrolled
## 2 108   male middle public  general   34    33   41      36    36 not enrolled
## 3  15   male   high public vocation   39    39   44      26    42 not enrolled
## 4  67   male    low public vocation   37    37   42      33    32 not enrolled
## 5 153   male middle public vocation   39    31   40      39    51 not enrolled
## 6  51 female   high public  general   42    36   42      31    39 not enrolled
##   awards cid
## 1      0   1
## 2      0   1
## 3      0   1
## 4      0   1
## 5      0   1
## 6      0   1
</code></pre>
<p>今回も、general, academic, vocation の3つの授業の中から生徒が選択した結果(prog)を出力（目的変数）、write(書く力を得点化したもの)と家庭の経済状況の指標ses(low, middle, highに分類)の2変数を入力(説明変数)とし、モデルを組み立てていきます。</p>
<h1 id="多項ロジスティックガウス過程モデル">多項ロジスティック・ガウス過程モデル</h1>
<h2 id="モデルの確認">モデルの確認</h2>
<p>出力を$(N×1)$ベクトル$\mathrm{y}=(y_1,\ldots,y_N)^T$、入力を$(N×(I+J))$行列$\mathrm{W}=(\mathrm{w}_1,\ldots, \mathrm{w}_N)^T$とします。</p>
<p>$$
\mathrm{w}_n = (\mathrm{x}_n^T,\mathrm{z}_n^T) \tag{1}
$$</p>
<p>$\mathrm{x}_n$、$\mathrm{z}_n$はそれぞれ要素数$I$の質的変数、要素数$J$の質的変数です。</p>
<p>$$
\left(x_{n1},\ldots,x_{nI}\right)^T=\mathrm{x}_{n}
$$</p>
<p>$$
\left(z_{n1},\ldots,w_{nJ}\right)^T=\mathrm{w}_{n}
$$</p>
<p>$\mathrm{y}$が$K$個の値をとるとき、$\mathrm{y}$が各値をとる確率を表す$(N×K)$行列$\mathrm{\Theta}=(\mathrm{\theta}_1, \ldots, \mathrm{\theta}_K)$を導入します。</p>
<p>ここで$(\theta_{k1},\ldots,\theta_{kN})^T=\mathrm{\theta}_k~~~(k=1,\ldots,K)$です。</p>
<p>$$
\mathrm{y} \sim Categorical(\Theta)\tag{2}
$$</p>
<p>$\Theta$は1行の要素の和をとると1になるので、各要素が$(-\infty,\infty)$の値をとる$(N×K)$行列$\mathrm{P}=(\mathrm{p}_1,\ldots,\mathrm{p}_K)$を導入し、下記のようにします。</p>
<p>ここで、$(p_{k1},\ldots,p_{kN})^T=\mathrm{p}_k~~(k=1,\ldots,K)$です。</p>
<p>$$
(\theta_{1n},\ldots,\theta_{Kn})^T = softmax(\mathrm{P}_n^T)~~~(n=1,\ldots,N) \tag{3}
$$</p>
<p>ここで$(p_{1n},p_{2n},\ldots,p_{Kn})=\mathrm{P}_n$です。</p>
<p>ガウス過程で推定する対象は、上記モデルの$\mathrm{P}$になります。多項ロジスティック回帰では、出力が任意の特定の値をとる確率を固定する必要があったので、それを踏まえて下記のようにおきます。</p>
<p>$$
\mathrm{p}_k = \begin{cases}
0~~~\mathrm{if}~k=1\\<br>
\mathrm{L}\eta_k~~~\mathrm{if}~k=2,\ldots,K
\end{cases}\tag{4}
$$</p>
<p>$$
\mathrm{LL^T} = \mathrm{K}\tag{5}
$$</p>
<p>$$
\eta_k=(\eta_{k1},\ldots,\eta_{kN}) \sim Normal(0,1) ~~(k=2,\ldots,K)\tag{6}
$$</p>
<p>ここで、$\mathrm{K}$は$(N×N)$のカーネル行列で、$(m,n)$成分をカーネル関数$k(\mathrm{w}_m,\mathrm{w}_n)$とします。カーネル関数の設定を工夫することで、線形モデルから得られた知見やデータの特徴(カテゴリカル変数が含まれる点)などをモデルに反映させていきます。</p>
<h2 id="カーネル関数の設定方法">カーネル関数の設定方法</h2>
<p>本記事では2つのカーネル関数を設定し、2段階に分けて多項ロジスティック回帰・ガウス過程モデルを実行します。</p>
<h3 id="モデル１-線形カーネル--isotropic-correlationカーネル">モデル１ 線形カーネル + isotropic correlationカーネル</h3>
<p>量的変数writeの影響は線形カーネルで取り入れ、カテゴリカル変数sesの影響をPDUDEである$\mathrm{T}$で考慮する、という構造です。$\mathrm{T}$の各要素はisotropic correlation functionで出力されます。
<!-- raw HTML omitted -->  <br>
このカーネル関数をロジスティック回帰・ガウス過程モデルに用いることで、これまでの記事<a href="https://rmorita-stat.github.io/my-page/post/multinom/multinom/">(1)</a>、<a href="https://rmorita-stat.github.io/my-page/post/multinom-rstan/multinom-rstan/">(2)</a>で得られた結果と同じ結果が得られることを確認し、モデルの妥当性チェックを行います。</p>
<p>カーネル関数は下になります。</p>
<p>$$
\theta_1\sum_{i=1}^{I}x_{mi}x_{ni} + \theta_2\sum_{j=1}^J\mathrm{T}(z_{nj},z_{mj}) = k(\mathrm{w}_m,\mathrm{w}_n) \tag{7}
$$</p>
<p>ここで、</p>
<p>$$
\mathrm{T} = \left(
\begin{array}{ccc}
1 &amp; c &amp; \cdots &amp; c \\<br>
c &amp; 1 &amp; \cdots &amp; c  \\<br>
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br>
c &amp; c &amp; \cdots &amp; 1
\end{array}
\right)\tag{8}
$$</p>
<p>は、<a href="https://rmorita-stat.github.io/my-page/post/gaussianprocess2/gaussianprocess2/">前回の記事</a>の式$(22)$、$(23)$あたりで説明したものす。</p>
<p>ハイパーパラメータは$\theta_1$、$\theta_2$、$c$の3つです。<br>
なお、$(7)$式第一項の線形カーネルの部分は、ガウス過程として扱わないで前回の記事の式$(1&rsquo;)$の$\beta\mathrm{f}(\mathrm{w})$で扱うこともできるのですが、今回はフルガウス過程で実行してみます。</p>
<h3 id="モデル２-線形カーネル--ガウスカーネル--isotropic-correlationカーネル">モデル２ 線形カーネル + (ガウスカーネル × isotropic correlationカーネル)</h3>
<p>量的変数writeの影響をモデル１のように線形的に把握するだけでなく、線形モデルでは誤差として扱われてしまう要素まで結果に取り入れるため、線形カーネルとガウスカーネルを組み合わせて量的変数の影響を予測します。質的変数の影響はモデル１と同様にisotropic correlationカーネルで予測します。</p>
<p>カーネル関数は下になります。</p>
<p>$$
\theta_1\sum_{i=1}^{I}x_{mi}x_{ni} +\theta_2exp\left(-\cfrac{1}{2\rho^2}\sum_{i=1}^{I}(x_{im}-x_{in})^2-\sum_{j=1}^{J}ln\left(\cfrac{1}{c}\right)I[r\neq{s}]\right)　\tag{9}
$$</p>
<p>ハイパーパラメータは$\theta_1$,$\theta2$,$\rho$,$c$の4つです。</p>
<h2 id="モデル１の実装と結果の確認">モデル１の実装と結果の確認</h2>
<p>モデル１を実行するStanコードを以下のようになります。</p>
<pre><code>//model2
data{
  int&lt;lower=2&gt; K; //カテゴリ数
  int&lt;lower=1&gt; N1; //データ数
  int N2; //予測入力数
  int&lt;lower=1&gt; I; //量的変数の数
  int&lt;lower=1&gt; J; //質的変数の数
  int M[J]; //各質的変数のカテゴリ数
  int&lt;lower=1, upper=K&gt; y[N1]; //出力ラベル
  int&lt;lower=1, upper=max(M)&gt; x1_j[N1,J]; //入力(質的変数)
  vector[I] x1_i[N1];//入力(量的変数)
  int&lt;lower=1, upper=max(M)&gt; x2_j[N2,J]; //予測したい点(質的変数)
  vector[I] x2_i[N2]; //予測したい点(量的変数)
}

transformed data{
  real delta = 1e-9;
  int&lt;lower=1&gt; N = N1 + N2;
  int&lt;lower=1, upper=max(M)&gt; x_j[N,J]; //入力(質的変数)と予測入力(質的変数)を縦に繋げたもの
  vector[I] x_i[N]; //入力(量的変数)と予測入力(量的変数)を縦に繋げたもの

  //以下でx1_iとx2_i,x1_jとx2_jを縦に結合し、新たな行列x_i,x_jをそれぞれ作成する
  for(n in 1:N1){
    for(j in 1:J){
      x_j[n,j] = x1_j[n,j];
    }
  }
  for(n in 1:N2){
    for(j in 1:J){
      x_j[(N1+n),j] = x2_j[n,j];
    }
  }
  for(n in 1:N1) x_i[n] = x1_i[n];  
  for(n in 1:N2) x_i[N1+n] = x2_i[n];


}

parameters{
  vector[N] eta[(K-1)]; //潜在変数
  vector&lt;lower=0&gt;[2] theta;
  vector&lt;lower=0, upper=1&gt;[J] C;
}

transformed parameters{
  vector[N] f[(K-1)];
  row_vector[K] p[N]; //ソフトマックス関数に投げる値

  //この中でカーネル関数を定義、ガウス過程の関数fを作成
  {
    matrix[N,N] L_K[(K-1)];
    matrix[N,N]linear_kernel;
    matrix[N,N]isotropic_correlation_kernel = rep_matrix(0,N,N);//初期値を0に設定。
    matrix[N,N] kernel_matrix[(K-1)];
    matrix[max(M),max(M)] PDUDE[J];

    //compound symmetric correlation matrix を作成
    for(j in 1:J){
      for(n in 1:(max(M)-1)){
      PDUDE[j][n,n] = 1;
      for(m in (n+1):max(M)){
        PDUDE[j][n,m] = C[j];
        PDUDE[j][m,n] = PDUDE[j][n,m];
        }
      }
      PDUDE[j][max(M),max(M)] = 1;
    }


    //isotropic_correlation_kernelを生成
    for(j in 1:J){
      for(n in 1:N){
        for(m in 1:N){
          isotropic_correlation_kernel[n,m] += PDUDE[j][x_j[n,j],x_j[m,j]];
        }
      }
    }

    //linear_kernel_matrixを生成。切片項はisotropic_correlation_kernelで捉えることにした
    for(n in 1:N){
      for(m in1:N){
        linear_kernel[n,m] = dot_product(x_i[n],x_i[m]);
      }
    }
    //kernel_matrixを作成
    for(k in 1:(K-1)){
      kernel_matrix[k] = theta[1] * linear_kernel + theta[2] * isotropic_correlation_kernel;
      for(n in 1:N){
        kernel_matrix[k][n,n] += delta;
      }
    }
    for(k in 1:(K-1)){
      L_K[k] = cholesky_decompose(kernel_matrix[k]);
      f[k] = L_K[k] * eta[k];
    }
  }

  // pの作成 1列目は0に固定 2列目以降に独立のガウス過程を指定
  for(n in 1:N){
     p[n,1] = 0;
     for(k in 1:(K-1)){
        p[n,(k+1)] = f[k,n];
     }
  }
}

model{
  //事前分布
  theta ~ student_t(4,0,2);
  C ~ normal(0,0.1);

  //モデル部分
  for(k in 1:(K-1)){
    for(n in 1:N){
      eta[k,n] ~ std_normal();
    }
  }
  for(n in 1:N1){
    y[n] ~ categorical_logit(p[n]');
  }

}

generated quantities{
  vector[K] pred[N2];
  for(n in 1:N2){
    pred[n] = softmax(p[(N1+n)]');
  }
}
</code></pre>
<p>入力データを命令する<code>data{}</code>ブロックでは、入力データ(<code>x1_i</code>、<code>x1_j</code>)を量的変数・質的変数で分けて指定しています。また、予測したい入力点(<code>x2_i</code>、<code>x2_j</code>)も指定しています。</p>
<p><code>transformes data{}</code>ブロックでは、式$(12)$の計算を実行するため、入力(実現値)と予測したい入力点のデータを量的変数・質的変数毎に縦に繋げています。</p>
<p><code>transformed parameter{}</code>ブロックでは、まず、式$(8)$の行列を<code>PDUDE</code>という呼称で作成しています。次に、<code>issotropic_correlation_kernel</code>として、<code>PDUDE</code>を参照しながらcompound symmetric correlation matrixを作成しています。
<!-- raw HTML omitted -->  <br>
また、<code>linear_kernel</code>として線形カーネルの要素を作成しています。線形カーネルはStanに実行された<code>dot_product()</code>が便利です。
<!-- raw HTML omitted -->  <br>
さらに、<code>kernel_matrix</code>として式$(7)$のカーネル関数を各要素に持つカーネル関数を作成します。
このように、線形カーネル・isotropic_correlation_kernelカーネルを作成し、それらを結合させてカーネル行列を作成し、最後に既述の方法でガウス過程に従う$\mathrm{f}$の定義までを<code>transformed parameter{}</code>で命令しています。</p>
<p><code>model{}</code>ブロックでは、まず事前分布を式$(7)$の$\theta_1$、$\theta_2$、式$(8)$の$c$で、適当に指定しています。
また、前回記事式$(9)$中段の設定を<code>eta ~ std_normal()</code>として指定し、多項ロジスティック回帰モデルの式$(2)$、式$(3)$を<code>y[n] ~ categorical_logit(p[n]')</code>としています。</p>
<p><code>generated quantities{}</code>ブロックでは、<code>model</code>ブロックでは作成しなかった入力点<code>x2_i</code>、<code>x2_j</code>における予測値を<code>pred</code>として指定しています。</p>
<p>このStanファイルを実行するコードは以下になります。</p>
<pre><code># progの3要素を数字に置き換えるための表を作成
progid &lt;- c(1,2,3)
names(progid) &lt;- c(&quot;academic&quot;,&quot;general&quot;,&quot;vocation&quot;)
# sesの3要素を数字に置き換えるための表を作成
sesid &lt;- c(1,2,3)
names(sesid) &lt;- c(&quot;low&quot;,&quot;middle&quot;,&quot;high&quot;)


library(makedummies)
library(tidyr)
library(dplyr)

d2 &lt;- ml %&gt;% mutate(sesid = sesid[paste(ses)]) %&gt;%
  mutate(progid = progid[paste(prog)]) %&gt;% mutate(write=(write-mean(write))/sd(write)) %&gt;% select(c(sesid, write, progid))

#x1:入力点
x1_j &lt;- data.frame(d2[,-c(2,3)]) # 入力(質的変数)
x1_i &lt;- data.frame(d2[,-c(1,3)]) # 入力(量的変数)
N1 &lt;- nrow(x1_i)
#x2:予測入力点
x2_j &lt;- data.frame(sesid = c(rep(1,41),rep(2,41),rep(3,41))) # 予測入力点(質的変数)
x2_i &lt;- data.frame(write = (rep(c(30:70),3)-mean(ml$write))/sd(ml$write)) # 予測入力点(量的変数)
N2 &lt;- nrow(x2_i)

#y:出力点
y &lt;- d2[,3]
K &lt;- 3
J &lt;- 1 #質的変数の数
I &lt;- 1 #量的変数の数
M &lt;- c(3) #各量的変数のカテゴリ数
dim(M) &lt;- 1

data &lt;- list(x1_i=x1_i,x1_j=x1_j,x2_i = x2_i,x2_j=x2_j,N1=N1,N2=N2,K=K,y=y,J=J,I=I, M=M)

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
fit_cate1 &lt;- stan(file=&quot;model2.stan&quot;, data=data, pars = c(&quot;pred&quot;,&quot;theta&quot;,&quot;C&quot;), warmup = 400, iter = 1500, chains=4, seed=1234)
</code></pre>
<p>計算にかかった時間はsurface laptop2 Corei5-8250Uで約3000秒でした。
ガウス過程は逆行列の計算などを含むので、工夫をしないとどうしても計算に時間がかかってしまうようです。</p>
<p>ハイパーパラメータの事後分布は以下のようになりました。</p>
<p><img src="/my-page/post/multinom-gp/fit_cate1_density.png" alt=""></p>
<p>予測結果は以下のようになりました。描画の為のコードは<a href="https://rmorita-stat.github.io/my-page/post/multinom-rstan/multinom-rstan/">以前の記事</a>を参照してください。</p>
<p><img src="/my-page/post/multinom-gp/fit_cate1_res.png" alt=""></p>
<p>rのパッケージnnetのmultinom()関数を使ったときや、Rstanで線形予測子Versionの多項ロジスティック回帰を実行したときの結果と同じ結果が得られていることが分かります。</p>
<p>以上、sotropic correlationカーネルを含んだモデル全体が正常に動くことを確認し、次の分析に進みます。</p>
<h2 id="モデル２の実装と結果の確認">モデル２の実装と結果の確認</h2>
<p>モデル２を実行するStanコードを以下のようになります。</p>
<pre><code>//model3
data{
  int&lt;lower=2&gt; K; //カテゴリ数
  int&lt;lower=1&gt; N1; //入力データの数
  int N2; //予測したい入力点の数
  int&lt;lower=1&gt; I; //量的変数の数
  int&lt;lower=1&gt; J; //質的変数の数
  int M[J]; //各質的変数のカテゴリ数
  int&lt;lower=1, upper=K&gt; y[N1]; //出力ラベル
  int&lt;lower=1, upper=max(M)&gt; x1_j[N1,J]; //入力(質的変数)
  vector[I] x1_i[N1];//入力(量的変数)
  int&lt;lower=1, upper=max(M)&gt; x2_j[N2,J]; //予測した入力点(質的変数)
  vector[I] x2_i[N2]; //予測したい入力点(量的変数)
}

transformed data{
  real delta = 1e-9;
  int&lt;lower=1&gt; N = N1 + N2;
  int&lt;lower=1, upper=max(M)&gt; x_j[N,J]; //入力(質的変数)と予測入力(質的変数)を縦に繋げたもの
  vector[I] x_i[N]; //入力(量的変数)と予測入力(量的変数)を縦に繋げたもの
  matrix[max(M),max(M)] inv_I; //対角成分が0、それ以外が1の行列を生成

  //以下でx1_iとx2_i,x1_jとx2_jを縦に結合し、新たな行列x_i,x_jをそれぞれ作成する
  for(n in 1:N1){
    for(j in 1:J){
      x_j[n,j] = x1_j[n,j];
    }
  }
  for(n in 1:N2){
    for(j in 1:J){
      x_j[(N1+n),j] = x2_j[n,j];
    }
  }
  for(n in 1:N1) x_i[n] = x1_i[n];  
  for(n in 1:N2) x_i[N1+n] = x2_i[n];

  //inv_Iの作成
  for(n in 1:(max(M)-1)){
    inv_I[n,n] = 0;
    for(m in (n+1):max(M)){
      inv_I[n,m] = 1;
      inv_I[m,n] = 1;
    }
  }
  inv_I[max(M),max(M)] = 0;

}

parameters{
  vector[N] eta[(K-1)]; //潜在変数
  vector&lt;lower=1&gt;[2] theta;線形カーネルと「ガウスカーネル・isotropic correlationカーネル」の重みを決定するハイパーパラメータ
  vector&lt;lower=0, upper=1&gt;[J] C; //isotropic correlationカーネルのハイパーパラメータ
  real&lt;lower=0, upper=0.9&gt; rho; //ガウスカーネルのshapeパラメータ
}

transformed parameters{
  vector[N] f[(K-1)];
  row_vector[K] p[N]; //ソフトマックス関数に投げる値


  //この中でカーネル関数を定義、ガウス過程に従う関数fを作成
  {
    matrix[N,N] L_K[(K-1)]; //カーネル行列をコレスキー分解した行列L。（出力がとる値）-1の数だけ準備する
    matrix[N,N]linear_kernel; //線形カーネル
    matrix[N,N]distance_L2_01_kernel; //。ガウスカーネル×isotropic correlation カーネル。logスケールでL2距離と0-1距離の成分を含んでいる
    matrix[N,N] kernel_matrix[(K-1)]; //最終的に作成するカーネル関数


    //linear_kernelを生成
    for(n in 1:N){
      for(m in1:N){
        linear_kernel[n,m] = dot_product(x_i[n],x_i[m]);
      }
    }

    //distanceL2_01_kernelを生成
    {
      matrix[N,N] distance_L2; //L2距離
      matrix[N,N] distance_01 = rep_matrix(0,N,N); //inv_Iに依存した0-1距離

      for(n in 1:N){
        for(m in 1:N){
          distance_L2[n,m] = dot_self((x_i[n]-x_i[m]) ./ rho);
        }
      }

      for(j in 1:J){
        for(n in 1:N){
          for(m in 1:N){
            distance_01[n,m] += log(1/C[j])*inv_I[x_j[n,j],x_j[m,j]];
          }
        }
      }

      for(n in 1:N){
        for(m in 1:N){
          distance_L2_01_kernel[n,m] = exp(-0.5 * distance_L2[n,m] -distance_01[n,m]);
        }
      }
    }


    //kernel_matrixを作成
    for(k in 1:(K-1)){
      kernel_matrix[k] = theta[1] * linear_kernel + theta[2] * distance_L2_01_kernel;
      for(n in 1:N){
        kernel_matrix[k][n,n] += delta;
      }
    }

    for(k in 1:(K-1)){
      L_K[k] = cholesky_decompose(kernel_matrix[k]);
      f[k] = L_K[k] * eta[k];
    }
  }

  // pの作成 1列目は0に固定 2列目以降に独立のガウス過程を指定
  for(n in 1:N){
     p[n,1] = 0;
     for(k in 1:(K-1)){
        p[n,(k+1)] = f[k,n];
     }
  }
}

model{
  //事前分布
  theta ~ student_t(4,1,2);
  rho ~ inv_gamma(0.1, 0.1);
  C ~ normal(0, 0.1);

  //モデル部分
  for(k in 1:(K-1)){
    for(n in 1:N){
      eta[k,n] ~ std_normal();
    }
  }
  for(n in 1:N1){
    y[n] ~ categorical_logit(p[n]');
  }

}

generated quantities{
  vector[K] pred[N2];
  for(n in 1:N2){
    pred[n] = softmax(p[(N1+n)]');
  }
}
</code></pre>
<p>モデル１との違いは、<code>transformed parameters{}</code>ブロックにおけるカーネル行列作成の部分と、<code>model{}</code>ブロックにおける事前分布設定の部分になります。</p>
<p><code>transformed parameters{}</code>ブロックでは、<code>linear_kernel</code>として式$(9)$第1項の部分（線形カーネル）の$\theta_1$を除いた部分を、<code>distance_L2_01_kernel</code>として式$(9)$第2項の$\theta_2$を除いた部分を作成しています。また<code>distance_L2_01_kernel</code>は、<code>distance_L2</code>として$\sum_{i=1}^{I}(x_{im}-x_{in})^2$を、<code>distance_01_kernel</code>として$\sum_{j=1}^{J}ln\left(\cfrac{1}{c}\right)I[r\neq{s}]$をそれぞれ作成し、それらを式$(9)$に従って合成することで作成しています。</p>
<p>ハイパーパラメータの事前分布の設定はかなり苦戦しました。最終的に以下の事前分布を採用しています。</p>
<p>$$
\theta_1 \sim Student\verb|_|t(4,1,2) ~~~\theta_1 \geq 1\\<br>
\theta_2 \sim Student\verb|_|t(4,1,2) ~~~\theta_2 \geq 1\\<br>
\rho \sim invGamma(0.1,0.1)~~~0 \leq \rho \leq 0.9  \\<br>
c \sim Normal(0,0.1)
$$</p>
<p>$\theta_1$、$\theta_2$については弱情報事前分布として、自由度4、期待値1、スケールパラメータ1の半t分布を採用しています。
<!-- raw HTML omitted -->  <br>
$\rho$については、Stanマニュアルを参照して、0付近の値を避けることができつつも小さな値にとがった山を持ち、かつ十分大きな値にも対応可能な逆ガンマ分布を採用しています。
<!-- raw HTML omitted -->  <br>
$c$については、0付近の値をとることが想定されるため、期待値0、標準偏差0.1の切断正規分布を採用しています。</p>
<p>様々な事前分布を試していたのですが、ガウスカーネルにおいて、shapeパラメータ$\rho$とrateパラメータ$\theta$の比が重要なようで、shapeパラメータに比べてrateパラメータが十分大きいと、出力値の変化の傾きが小さくなり、前回記事の式$(1)$の誤差項を捉えてくれなくなってしまいます。この点についてはStanマニュアルにも以下の記載があります。</p>
<blockquote>
<p>Perhaps most importantly, the parameter $\rho$ and $\alpha$ are weakly identified Zhang(2004). The ratio of the rwo parameters is well-identified&hellip;</p>
</blockquote>
<p>今回の場合、$\rho$に値の上限を設定しないとどうしても$\rho$が大きくなってしまい、予測結果もモデル式１と変わらなくなってしまいました。しかしそれはこのモデルの意図した結果ではありません。</p>
<p>以前の<a href="https://rmorita-stat.github.io/my-page/post/gaussianprocessori/gaussianprocess/">ガウス過程のシミュレーション結果</a>を見ると、標準化されたデータの場合、shapeパラメータ、rateパラメータともに1前後でちょうどよいガウス過程からの出力が得られそうであることが確認できます。よって、今回はrateパラメータに0.9の上限を設け、shapeパラメータも下限値1を設定することで、少し変化の傾きが急な出力を得られるように強要することにします。</p>
<p>上記のStanファイルを実行するコードは以下になります。</p>
<pre><code>data &lt;- list(x1_i=x1_i,x1_j=x1_j,x2_i = x2_i,x2_j=x2_j,N1=N1,N2=N2,K=K,y=y,J=J,I=I, M=M)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
fit_cate2 &lt;- stan(file=&quot;model3.stan&quot;, data=data, pars = c(&quot;pred&quot;,&quot;rho&quot;,&quot;theta&quot;,&quot;C&quot;), warmup = 400, iter = 1500, chains=4, seed=1234)
</code></pre>
<p>計算にかかったた時間は確かおよそ9800秒でした。2時間以上かかっていますね。PCもうなり声をあげていたので冷却対策など必要かもしれません。</p>
<p>ハイパーパラメータの事後分布は以下のようになります。</p>
<pre><code>posterior_fit0_2 &lt;- rstan::extract(fit_cate2)
library(bayesplot)
plot_title &lt;- ggtitle(&quot;Posterior distribution of hyper parameters&quot;, &quot;with medians and 95% intervals&quot;)
p1 &lt;- mcmc_areas(as.matrix(fit_cate2),
                regex_pars = c(&quot;theta&quot;),prob=0.95, area_method = &quot;equal height&quot;) +
  scale_y_discrete(labels=c(&quot;theta1&quot;,&quot;theta2&quot;)) + theme_bw(base_size=12)
p2 &lt;- mcmc_areas(as.matrix(fit_cate2),
                regex_pars = c(&quot;C&quot;,&quot;rho&quot;),prob=0.95, area_method = &quot;equal height&quot;) +
  scale_y_discrete(labels=c(&quot;c&quot;,&quot;rho&quot;)) +
   theme_bw(base_size=12)
library(gridExtra)

g1 &lt;- ggplot_gtable(ggplot_build(p1))
g2 &lt;- ggplot_gtable(ggplot_build(p2))
Width &lt;- unit.pmax(g1$widths, g2$widths)
Height &lt;- unit.pmin(g1$heights, g2$heights)
g1$widths &lt;- Width
g2$widths &lt;- Width
g1$heights &lt;- Height
g2$heights &lt;- Height
p &lt;- gridExtra::grid.arrange(g1,g2,ncol=1, top=&quot;Posterior distribution of hyper parameters (with medians and 95% intervals)&quot;)
</code></pre>
<p><img src="/my-page/post/multinom-gp/fit_cate2_density.png" alt=""></p>
<p>$\rho$が頑張って大きな値をとろうとしている様子が見えますが、そこは抑えてもらっています。何だかかわいそう&hellip;</p>
<p>最後に、予測結果を描画します。</p>
<p><img src="/my-page/post/multinom-gp/fit_cate2_res.png" alt=""></p>
<p>線形モデルでは読み取ることが出来なかった傾向がうまく捉えられています。例えば、読み書きの能力が50前後のses=lowの生徒はgeneralの授業をとる傾向にあること、読み書きの能力が45前後のses=highの生徒はacademicの授業をとる確率とvocationの授業をとる確率が同程度である様子などが確認できます。</p>
<p><strong>It&rsquo;s so brilliant</strong>&gt;🐢</p>
<p>このように、線形モデルでは誤差として結果に反映されなかった事象もうまくとらえることが出来るのがガウス過程の魅力です。パラメータの事後分布や推測値よりデータの生成過程を考察する、という目的には不向きかもしれませんが、予測の観点から見れば非常に便利ではないでしょうか？。</p>
<h1 id="まとめ">まとめ</h1>
<p>今回はガウス過程を多項ロジスティック回帰に取り込んだモデルの実装を行いました。その過程で、ガウス過程潜在変数モデルやカテゴリカルデータを取り入れたガウス過程等、前回の記事で説明したガウス過程の応用手法を用い、それらが正常に機能することを確かめました。</p>
<p>また、ガウス過程のように特定の分布を想定しないで、データに応じてモデルの複雑さを決定するパラメータを調整するベイズモデルは<strong>ノンパラメトリックベイズモデル</strong>と呼ばれています。</p>
<p>この記事で、以前に述べたガウス過程の活用方法の１つ目(下記)を紹介した形です。</p>
<ul>
<li>一般化線形モデルの線形予測子をガウス過程に置き換え、柔軟なモデルに豹変させる</li>
</ul>
<p>多項ロジスティック回帰の記事も3つ目になりましたが、これで最後になります。最近はアウトプットに力を注いでいて投稿頻度も多かったですが、今後暫くはインプットに集中したいと考えており、投稿しない月が続くかもしれません。</p>
<p>Enjoy Stan!</p>

        </article>
    </div>
    <div class="main-content__tags u-font">
        
        
        <span><a href="https://rmorita-stat.github.io/my-page/tags/%E5%9B%9E%E5%B8%B0">回帰</a></span>
        
        <span><a href="https://rmorita-stat.github.io/my-page/tags/%E3%82%AC%E3%82%A6%E3%82%B9%E9%81%8E%E7%A8%8B">ガウス過程</a></span>
        
        <span><a href="https://rmorita-stat.github.io/my-page/tags/rstan">rstan</a></span>
        
        <span><a href="https://rmorita-stat.github.io/my-page/tags/%E3%83%8E%E3%83%B3%E3%83%91%E3%83%A9%E3%83%A1%E3%83%88%E3%83%AA%E3%83%83%E3%82%AF%E3%83%99%E3%82%A4%E3%82%BA%E3%83%A2%E3%83%87%E3%83%AB">ノンパラメトリックベイズモデル</a></span>
        
        
    </div>
</div>
<div class="main-profile">
    <div class="main-profile__avatar">
        
    </div>
    <div class="main-profile__body">
        <div class="main-profile__author">
            
            <span> R.morita </span>
            
        </div>
        <div class="main-profile__description">
            
            <p> 洛中で6年間大学生活を過ごし、今は難波の地で働いています。統計、ロードバイク、古墳が好きです。 </p>
            
        </div>
    </div>
</div>
<div class="main-line"></div>
<div class="main-pn">
    
    <a class="previous" href="https://rmorita-stat.github.io/my-page/post/gaussianprocess2/gaussianprocess2/">
        <div class="pn-dec"></div>
        <div class="pn-els">
            <div class="pn-el__1"> 2020.07.11 00:00 </div>
            <div class="pn-el__2"> ガウス過程の応用 </div>
        </div>
    </a>
    
    
    <a class="next" href="https://rmorita-stat.github.io/my-page/post/modeling/modeling/">
        <div class="pn-dec"></div>
        <div class="pn-els">
            <div class="pn-el__1"> 2020.10.03 00:00 </div>
            <div class="pn-el__2"> モデリングと情報量基準その１～モデリングとは？～ </div>
        </div>
    </a>
    
</div>

<footer>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
    });
</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  "HTML-CSS": {
    availableFonts: ["TeX"]
  }
  });
</script>
</footer>

</div>
<div class="footer">
    <div class="copyright-wrap">
        <p class="copyright u-font">
            
            &#169;
            2020
            
            <a href="https://github.com/Rmorita-stat/doc" target="_blank">R.morita&#46;</a>
            Theme <a href="https://github.com/iCyris/hugo-theme-yuki" target="_blank">yuki</a>&#46;
            Powered by Hugo&#46;
            
            
        </p>
    </div>
</div>
</body>
<script src="https://rmorita-stat.github.io/my-page/js/page.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

